#!/usr/bin/env python3

import re
import json
import os
from datetime import datetime, timedelta
from collections import Counter, defaultdict
from pathlib import Path
import argparse
import sys

class DnsmasqAnalyzer:
    def __init__(self, log_file='/var/log/dnsmasq.log', data_dir='./dnsmasq_data', keep_days=30, exclude_arpa=True):
        self.log_file = log_file
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        self.keep_days = keep_days  # ä¿ç•™å¤©æ•°ï¼Œé»˜è®¤30å¤©
        self.exclude_arpa = exclude_arpa  # æ˜¯å¦æ’é™¤.arpaåŸŸå
        
        # çŠ¶æ€æ–‡ä»¶ï¼Œè®°å½•ä¸Šæ¬¡å¤„ç†çš„ä½ç½®
        self.state_file = self.data_dir / '.last_processed_state.json'
        self.last_processed_time = None
        self.load_state()
        
        # æ­£åˆ™è¡¨è¾¾å¼è§£ædnsmasqæ—¥å¿—
        # æ ¼å¼: Dec 30 12:30:45 dnsmasq[12345]: query[A] domain.com from 192.168.1.1
        self.log_pattern = re.compile(
            r'(\w+\s+\d+\s+\d+:\d+:\d+).*?query\[(\w+)\]\s+([^\s]+)\s+from\s+([^\s]+)'
        )
        
        # ç¼“å­˜å‘½ä¸­çš„æ­£åˆ™è¡¨è¾¾å¼
        # æ ¼å¼: Dec 30 12:30:45 dnsmasq[12345]: cached domain.com is 1.2.3.4
        self.cache_pattern = re.compile(
            r'(\w+\s+\d+\s+\d+:\d+:\d+).*?cached\s+([^\s]+)\s+'
        )
        
        # è½¬å‘æŸ¥è¯¢çš„æ­£åˆ™è¡¨è¾¾å¼
        # æ ¼å¼: Dec 30 12:30:45 dnsmasq[12345]: forwarded domain.com to 8.8.8.8
        # æˆ–: Dec 30 12:30:45 dnsmasq[12345]: forwarded domain.com to 8.8.8.8#53
        # æˆ–: Dec 30 12:30:45 dnsmasq[12345]: forwarded domain.com to 192.168.1.1#5353
        self.forward_pattern = re.compile(
            r'(\w+\s+\d+\s+\d+:\d+:\d+).*?forwarded\s+([^\s]+)\s+to\s+([^\s]+(?:#\d+)?)'
        )
        
        # ç”¨äºå­˜å‚¨å½“å¤©æ•°æ®
        self.today_data = {
            'date': datetime.now().strftime('%Y-%m-%d'),
            'queries': [],
            'domain_counts': Counter(),
            'query_types': Counter(),
            'client_ips': Counter(),
            'hourly_stats': defaultdict(int),
            'cache_hits': 0,
            'cache_misses': 0,
            'cached_domains': Counter(),
            'forwarded_domains': Counter(),
            'upstream_servers': Counter(),
            'hourly_cache_stats': defaultdict(lambda: {'hits': 0, 'misses': 0}),
            'processed_lines': set(),  # ç”¨äºå»é‡
            'arpa_queries_excluded': 0  # æ’é™¤çš„.arpaæŸ¥è¯¢æ•°é‡
        }
        
        # æ•°æ®æ¢å¤çŠ¶æ€æ ‡å¿—
        self._data_restored = False
        
        # åŠ è½½ä»Šå¤©å·²æœ‰çš„æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        self.load_existing_data()
        
    def is_arpa_domain(self, domain):
        """æ£€æŸ¥æ˜¯å¦ä¸º.arpaåŸŸåï¼ˆåå‘DNSæŸ¥è¯¢ï¼‰"""
        return domain.endswith('.arpa')
    
    def is_within_analysis_window(self, timestamp):
        """æ™ºèƒ½çš„æ—¶é—´çª—å£æ£€æŸ¥ï¼Œå¤„ç†è¾¹ç•Œæ¡ä»¶"""
        now = datetime.now()
        today_start = now.replace(hour=0, minute=0, second=0, microsecond=0)
        tomorrow_start = today_start + timedelta(days=1)
        
        # æ£€æŸ¥æ˜¯å¦åœ¨å½“å¤©èŒƒå›´å†…ï¼ˆä»ä»Šå¤©00:00åˆ°ç°åœ¨ï¼‰
        if today_start <= timestamp <= now:
            return True
        
        # å¤„ç†è·¨å¤©æƒ…å†µï¼šå¦‚æœå½“å‰æ—¶é—´æ˜¯å‡Œæ™¨ï¼Œå¯èƒ½éœ€è¦åŒ…å«æ˜¨å¤©æ™šä¸Šçš„æ—¥å¿—
        if now.hour < 2:  # å‡Œæ™¨2ç‚¹å‰
            yesterday_22 = today_start - timedelta(hours=2)  # æ˜¨å¤©22ç‚¹
            if yesterday_22 <= timestamp < today_start:
                return True
        
        # æ‰©å±•çª—å£ï¼šåŒ…å«æœ€è¿‘7å¤©çš„æ•°æ®ï¼ˆç”¨äºå†å²æ—¥å¿—åˆ†æï¼‰
        week_ago = today_start - timedelta(days=7)
        if week_ago <= timestamp < today_start:
            return True
        
        # å¤„ç†æ—¶é—´æˆ³ç•¥å¾®è¶…å‰çš„æƒ…å†µï¼ˆå¯èƒ½çš„ç³»ç»Ÿæ—¶é—´å·®å¼‚ï¼‰ï¼Œä½†ä¸è¶…è¿‡æ˜å¤©
        if now < timestamp <= min(now + timedelta(minutes=10), tomorrow_start):
            return True
            
        return False
        
    def parse_timestamp(self, timestamp_str):
        """å¥å£®çš„æ—¶é—´æˆ³è§£ææ–¹æ³•"""
        current_year = datetime.now().year
        now = datetime.now()
        
        # å°è¯•å¤šç§æ—¶é—´æˆ³æ ¼å¼
        timestamp_formats = [
            # ä¸åŒ…å«å¹´ä»½çš„æ ¼å¼ï¼ˆæœ€å¸¸è§ï¼‰
            "%b %d %H:%M:%S",                     # Aug 16 05:00:06
            "%B %d %H:%M:%S",                     # August 16 05:00:06
            # åŒ…å«å¹´ä»½çš„æ ¼å¼
            "%Y %b %d %H:%M:%S",                  # 2025 Aug 16 05:00:06
            "%Y %B %d %H:%M:%S",                  # 2025 August 16 05:00:06
            "%Y-%m-%d %H:%M:%S",                  # 2025-08-16 05:00:06
        ]
        
        for fmt in timestamp_formats:
            try:
                # æ ¹æ®æ—¶é—´æˆ³æ˜¯å¦åŒ…å«å¹´ä»½æ¥å†³å®šå¤„ç†æ–¹å¼
                if timestamp_str.strip().startswith(('19', '20')):  # åŒ…å«å¹´ä»½
                    parsed_time = datetime.strptime(timestamp_str, fmt)
                else:  # ä¸åŒ…å«å¹´ä»½ï¼Œéœ€è¦æ·»åŠ å½“å‰å¹´ä»½
                    if "%Y" in fmt:
                        # è·³è¿‡åŒ…å«å¹´ä»½çš„æ ¼å¼ï¼Œå› ä¸ºæ—¶é—´æˆ³ä¸åŒ…å«å¹´ä»½
                        continue
                    # è§£æä¸å«å¹´ä»½çš„æ—¶é—´æˆ³
                    parsed_time = datetime.strptime(timestamp_str, fmt)
                    # æ‰‹åŠ¨æ·»åŠ å¹´ä»½
                    parsed_time = parsed_time.replace(year=current_year)
                
                # æ£€æŸ¥è§£æçš„æ—¶é—´æ˜¯å¦åˆç†ï¼ˆä¸è¶…è¿‡å½“å‰æ—¶é—´å¤ªè¿œï¼‰
                time_diff = abs((now - parsed_time).total_seconds())
                if time_diff > 366 * 24 * 3600:  # è¶…è¿‡ä¸€å¹´
                    # å°è¯•ä½¿ç”¨å‰ä¸€å¹´
                    try:
                        adjusted_time = parsed_time.replace(year=current_year - 1)
                        if abs((now - adjusted_time).total_seconds()) <= 366 * 24 * 3600:
                            return adjusted_time
                    except ValueError:
                        pass
                    # å¦‚æœè°ƒæ•´å¹´ä»½åè¿˜æ˜¯ä¸åˆç†ï¼Œç»§ç»­å°è¯•å…¶ä»–æ ¼å¼
                    continue
                
                return parsed_time
            except ValueError:
                continue
        
        # æœ€åçš„fallbackï¼šä½¿ç”¨å½“å‰æ—¶é—´ï¼Œä½†è®°å½•è­¦å‘Š
        print(f"è­¦å‘Šï¼šæ— æ³•è§£ææ—¶é—´æˆ³ '{timestamp_str}'ï¼Œä½¿ç”¨å½“å‰æ—¶é—´")
        return now

    def parse_log_line(self, line):
        """è§£æå•è¡Œæ—¥å¿—"""
        # æ£€æŸ¥æ˜¯å¦æ˜¯æŸ¥è¯¢è®°å½•
        match = self.log_pattern.search(line)
        if match:
            timestamp_str, query_type, domain, client_ip = match.groups()
            timestamp = self.parse_timestamp(timestamp_str)
            
            return {
                'type': 'query',
                'timestamp': timestamp,
                'query_type': query_type,
                'domain': domain.lower(),
                'client_ip': client_ip,
                'hour': timestamp.hour
            }
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç¼“å­˜å‘½ä¸­è®°å½•
        cache_match = self.cache_pattern.search(line)
        if cache_match:
            timestamp_str, domain = cache_match.groups()
            timestamp = self.parse_timestamp(timestamp_str)
            
            return {
                'type': 'cache_hit',
                'timestamp': timestamp,
                'domain': domain.lower(),
                'hour': timestamp.hour
            }
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯è½¬å‘è®°å½•
        forward_match = self.forward_pattern.search(line)
        if forward_match:
            timestamp_str, domain, upstream = forward_match.groups()
            timestamp = self.parse_timestamp(timestamp_str)
            
            return {
                'type': 'forward',
                'timestamp': timestamp,
                'domain': domain.lower(),
                'upstream': upstream,
                'hour': timestamp.hour
            }
        
        return None
    
    def load_state(self):
        """åŠ è½½ä¸Šæ¬¡å¤„ç†çŠ¶æ€"""
        if self.state_file.exists():
            try:
                with open(self.state_file, 'r') as f:
                    state = json.load(f)
                    self.last_processed_time = datetime.fromisoformat(state.get('last_processed_time', ''))
                    print(f"ä¸Šæ¬¡å¤„ç†æ—¶é—´: {self.last_processed_time}")
            except Exception as e:
                print(f"åŠ è½½çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}")
                self.last_processed_time = None
    
    def save_state(self):
        """ä¿å­˜å¤„ç†çŠ¶æ€"""
        state = {
            'last_processed_time': datetime.now().isoformat(),
            'log_file': self.log_file
        }
        with open(self.state_file, 'w') as f:
            json.dump(state, f, indent=2)
    
    def load_existing_data(self):
        """åŠ è½½ä»Šå¤©å·²æœ‰çš„æ•°æ®ï¼Œé¿å…é‡å¤ç»Ÿè®¡"""
        date_str = datetime.now().strftime('%Y-%m-%d')
        data_file = self.data_dir / f"dns_data_{date_str}.json"
        
        if data_file.exists():
            try:
                with open(data_file, 'r') as f:
                    existing_data = json.load(f)
                    
                # æ¢å¤å·²æœ‰çš„ç»Ÿè®¡æ•°æ®
                self.today_data['cache_hits'] = existing_data.get('cache_hits', 0)
                self.today_data['cache_misses'] = existing_data.get('cache_misses', 0)
                self.today_data['arpa_queries_excluded'] = existing_data.get('arpa_queries_excluded', 0)
                self.today_data['domain_counts'].update(existing_data.get('domain_counts', {}))
                self.today_data['query_types'].update(existing_data.get('query_types', {}))
                self.today_data['client_ips'].update(existing_data.get('client_ips', {}))
                self.today_data['cached_domains'].update(existing_data.get('cached_domains', {}))
                self.today_data['forwarded_domains'].update(existing_data.get('forwarded_domains', {}))
                self.today_data['upstream_servers'].update(existing_data.get('upstream_servers', {}))
                
                # æ¢å¤å°æ—¶ç»Ÿè®¡
                for hour, stats in existing_data.get('hourly_cache_stats', {}).items():
                    self.today_data['hourly_cache_stats'][int(hour)] = stats
                for hour, count in existing_data.get('hourly_stats', {}).items():
                    self.today_data['hourly_stats'][int(hour)] = count
                    
                # æ¢å¤å·²å¤„ç†çš„è¡Œå“ˆå¸Œï¼ˆç”¨äºå»é‡ï¼‰
                if 'processed_lines_hash' in existing_data:
                    self.today_data['processed_lines'] = set(existing_data['processed_lines_hash'])
                
                # æ ¹æ®æ€»æŸ¥è¯¢æ•°é‡å»ºqueriesåˆ—è¡¨ï¼ˆç”¨äºç»Ÿè®¡ç›®çš„ï¼‰
                total_queries = existing_data.get('total_queries', 0)
                self.today_data['queries'] = [{'type': 'query'}] * total_queries  # ç®€åŒ–çš„æŸ¥è¯¢è®°å½•
                
                # è®°å½•å·²æ¢å¤çš„çŠ¶æ€ï¼Œé¿å…é‡å¤è®¡ç®—
                self._data_restored = True
                    
                print(f"å·²åŠ è½½ä»Šå¤©çš„ç°æœ‰æ•°æ®ï¼Œç»§ç»­å¢é‡ç»Ÿè®¡")
            except Exception as e:
                print(f"åŠ è½½ç°æœ‰æ•°æ®å¤±è´¥: {e}")
    
    def get_line_hash(self, line):
        """ç”Ÿæˆæ—¥å¿—è¡Œçš„å”¯ä¸€æ ‡è¯†ç¬¦ï¼ˆåŒ…å«æ—¶é—´æˆ³å’Œæ–‡ä»¶ä½ç½®ä¿¡æ¯å¢å¼ºå”¯ä¸€æ€§ï¼‰"""
        import hashlib
        
        # æå–æ—¶é—´æˆ³ä¿¡æ¯ä»¥å¢å¼ºå”¯ä¸€æ€§
        timestamp_info = ""
        parsed_data = self.parse_log_line(line)
        if parsed_data and 'timestamp' in parsed_data:
            timestamp_info = parsed_data['timestamp'].isoformat()
        
        # ç»“åˆåŸå§‹è¡Œå†…å®¹ã€æ—¶é—´æˆ³å’Œè¡Œé•¿åº¦åˆ›å»ºæ›´å¼ºçš„å”¯ä¸€æ ‡è¯†
        unique_string = f"{line.strip()}|{timestamp_info}|{len(line)}"
        return hashlib.sha256(unique_string.encode()).hexdigest()[:32]  # ä½¿ç”¨SHA256å¹¶æˆªå–å‰32ä½
    
    def analyze_log(self):
        """åˆ†ææ—¥å¿—æ–‡ä»¶"""
        if not os.path.exists(self.log_file):
            print(f"æ—¥å¿—æ–‡ä»¶ {self.log_file} ä¸å­˜åœ¨")
            return False
        
        new_records = 0
        duplicate_records = 0
        
        try:
            with open(self.log_file, 'r', encoding='utf-8', errors='ignore') as f:
                for line in f:
                    # ç”Ÿæˆè¡Œçš„å”¯ä¸€æ ‡è¯†
                    line_hash = self.get_line_hash(line.strip())
                    
                    # è·³è¿‡å·²å¤„ç†çš„è¡Œ
                    if line_hash in self.today_data['processed_lines']:
                        duplicate_records += 1
                        continue
                    
                    data = self.parse_log_line(line)
                    if data and self.is_within_analysis_window(data['timestamp']):
                        # è®°å½•å·²å¤„ç†çš„è¡Œ
                        self.today_data['processed_lines'].add(line_hash)
                        new_records += 1
                        
                        # å¤„ç†æŸ¥è¯¢è®°å½•
                        if data['type'] == 'query':
                            # æ£€æŸ¥æ˜¯å¦ä¸º.arpaåŸŸåå¹¶æ ¹æ®è®¾ç½®å†³å®šæ˜¯å¦æ’é™¤
                            if self.exclude_arpa and self.is_arpa_domain(data['domain']):
                                self.today_data['arpa_queries_excluded'] += 1
                            else:
                                self.today_data['queries'].append(data)
                                self.today_data['domain_counts'][data['domain']] += 1
                                self.today_data['query_types'][data['query_type']] += 1
                                self.today_data['client_ips'][data['client_ip']] += 1
                                self.today_data['hourly_stats'][data['hour']] += 1
                        
                        # å¤„ç†ç¼“å­˜å‘½ä¸­è®°å½•
                        elif data['type'] == 'cache_hit':
                            # æ£€æŸ¥æ˜¯å¦ä¸º.arpaåŸŸåå¹¶æ ¹æ®è®¾ç½®å†³å®šæ˜¯å¦æ’é™¤
                            if self.exclude_arpa and self.is_arpa_domain(data['domain']):
                                self.today_data['arpa_queries_excluded'] += 1
                            else:
                                self.today_data['cache_hits'] += 1
                                self.today_data['cached_domains'][data['domain']] += 1
                                self.today_data['hourly_cache_stats'][data['hour']]['hits'] += 1
                        
                        # å¤„ç†è½¬å‘è®°å½•ï¼ˆç¼“å­˜æœªå‘½ä¸­ï¼‰
                        elif data['type'] == 'forward':
                            # æ£€æŸ¥æ˜¯å¦ä¸º.arpaåŸŸåå¹¶æ ¹æ®è®¾ç½®å†³å®šæ˜¯å¦æ’é™¤
                            if self.exclude_arpa and self.is_arpa_domain(data['domain']):
                                self.today_data['arpa_queries_excluded'] += 1
                            else:
                                self.today_data['cache_misses'] += 1
                                self.today_data['forwarded_domains'][data['domain']] += 1
                                self.today_data['upstream_servers'][data['upstream']] += 1
                                self.today_data['hourly_cache_stats'][data['hour']]['misses'] += 1
            
            # æ¸…ç†è¿‡æœŸçš„è¡Œå“ˆå¸Œè®°å½•
            self.cleanup_processed_lines_hash()
            
            # è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡
            total_lookups = self.today_data['cache_hits'] + self.today_data['cache_misses']
            if total_lookups > 0:
                cache_hit_rate = (self.today_data['cache_hits'] / total_lookups) * 100
            else:
                cache_hit_rate = 0
            
            print(f"\nç»Ÿè®¡ç»“æœ:")
            print(f"  æ–°å¢è®°å½•: {new_records} æ¡")
            print(f"  è·³è¿‡é‡å¤: {duplicate_records} æ¡")
            if self.exclude_arpa:
                print(f"  æ’é™¤.arpaæŸ¥è¯¢: {self.today_data['arpa_queries_excluded']} æ¡")
            print(f"  æŸ¥è¯¢æ€»æ•°: {len(self.today_data['queries'])} æ¡")
            print(f"  ç¼“å­˜å‘½ä¸­: {self.today_data['cache_hits']} æ¬¡")
            print(f"  ç¼“å­˜æœªå‘½ä¸­: {self.today_data['cache_misses']} æ¬¡")
            print(f"  ç¼“å­˜å‘½ä¸­ç‡: {cache_hit_rate:.2f}%")
            
            # æ˜¾ç¤ºæ•°æ®ç›®å½•çŠ¶æ€
            total_size, file_count = self.get_data_directory_size()
            size_mb = total_size / 1024 / 1024
            print(f"  æ•°æ®ç›®å½•çŠ¶æ€: {file_count} ä¸ªæ–‡ä»¶ï¼Œæ€»å¤§å° {size_mb:.2f} MB")
            
            # ä¿å­˜å¤„ç†çŠ¶æ€
            self.save_state()
            
            return True
            
        except Exception as e:
            print(f"è¯»å–æ—¥å¿—æ–‡ä»¶å‡ºé”™: {e}")
            return False
    
    def save_daily_data(self):
        """ä¿å­˜å½“å¤©æ•°æ®åˆ°JSONæ–‡ä»¶"""
        date_str = datetime.now().strftime('%Y-%m-%d')
        data_file = self.data_dir / f"dns_data_{date_str}.json"
        
        # è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡
        total_lookups = self.today_data['cache_hits'] + self.today_data['cache_misses']
        cache_hit_rate = (self.today_data['cache_hits'] / total_lookups * 100) if total_lookups > 0 else 0
        
        # å‡†å¤‡è¦ä¿å­˜çš„æ•°æ®
        save_data = {
            'date': date_str,
            'total_queries': len(self.today_data['queries']),
            'arpa_queries_excluded': self.today_data['arpa_queries_excluded'],
            'domain_counts': dict(self.today_data['domain_counts']),
            'query_types': dict(self.today_data['query_types']),
            'client_ips': dict(self.today_data['client_ips']),
            'hourly_stats': dict(self.today_data['hourly_stats']),
            'top_domains': dict(self.today_data['domain_counts'].most_common(100)),
            'cache_hits': self.today_data['cache_hits'],
            'cache_misses': self.today_data['cache_misses'],
            'cache_hit_rate': cache_hit_rate,
            'cached_domains': dict(self.today_data['cached_domains'].most_common(100)),
            'forwarded_domains': dict(self.today_data['forwarded_domains'].most_common(100)),
            'upstream_servers': dict(self.today_data['upstream_servers']),
            'hourly_cache_stats': dict(self.today_data['hourly_cache_stats']),
            # ä¿å­˜å·²å¤„ç†è¡Œçš„å“ˆå¸Œå€¼åˆ—è¡¨ï¼ˆé™åˆ¶å¤§å°é¿å…æ–‡ä»¶è¿‡å¤§ï¼‰
            'processed_lines_hash': list(self.today_data['processed_lines'])[-10000:] if len(self.today_data['processed_lines']) > 10000 else list(self.today_data['processed_lines']),
            'last_update': datetime.now().isoformat()
        }
        
        with open(data_file, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, indent=2, ensure_ascii=False)
        
        print(f"æ•°æ®å·²ä¿å­˜åˆ° {data_file}")
        
        # æ‰§è¡Œæ•°æ®æ–‡ä»¶æ¸…ç†
        self.cleanup_old_data()
    
    def cleanup_old_data(self):
        """æ¸…ç†è¿‡æœŸçš„æ•°æ®æ–‡ä»¶"""
        try:
            current_time = datetime.now()
            cleanup_count = 0
            total_size_cleaned = 0
            
            # æ‰«ææ•°æ®ç›®å½•ä¸­çš„æ‰€æœ‰jsonæ–‡ä»¶
            for file_path in self.data_dir.glob("dns_data_*.json"):
                try:
                    # ä»æ–‡ä»¶åæå–æ—¥æœŸ
                    file_name = file_path.stem
                    if file_name.startswith('dns_data_'):
                        date_str = file_name.replace('dns_data_', '')
                        file_date = datetime.strptime(date_str, '%Y-%m-%d')
                        
                        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡ä¿ç•™æœŸé™
                        days_old = (current_time - file_date).days
                        if days_old > self.keep_days:
                            file_size = file_path.stat().st_size
                            file_path.unlink()  # åˆ é™¤æ–‡ä»¶
                            cleanup_count += 1
                            total_size_cleaned += file_size
                            print(f"  å·²åˆ é™¤è¿‡æœŸæ•°æ®æ–‡ä»¶: {file_path.name} ({days_old}å¤©å‰)")
                            
                except (ValueError, OSError) as e:
                    print(f"  å¤„ç†æ–‡ä»¶ {file_path.name} æ—¶å‡ºé”™: {e}")
                    continue
            
            if cleanup_count > 0:
                size_mb = total_size_cleaned / 1024 / 1024
                print(f"æ•°æ®æ¸…ç†å®Œæˆ: åˆ é™¤äº† {cleanup_count} ä¸ªæ–‡ä»¶ï¼Œé‡Šæ”¾ç©ºé—´ {size_mb:.2f} MB")
            else:
                print(f"æ•°æ®æ¸…ç†æ£€æŸ¥å®Œæˆ: å½“å‰æ‰€æœ‰æ–‡ä»¶éƒ½åœ¨ {self.keep_days} å¤©ä¿ç•™æœŸå†…")
                
        except Exception as e:
            print(f"æ•°æ®æ¸…ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
    
    def cleanup_processed_lines_hash(self):
        """æ¸…ç†è¿‡æœŸçš„å·²å¤„ç†è¡Œå“ˆå¸Œå€¼ï¼Œé¿å…å†…å­˜å’Œå­˜å‚¨ç©ºé—´è¿‡åº¦å ç”¨"""
        # æ›´æ™ºèƒ½çš„æ¸…ç†ç­–ç•¥ï¼šåŸºäºæ—¶é—´æˆ³è¿›è¡Œæ¸…ç†ï¼Œè€Œä¸æ˜¯ç®€å•çš„æ•°é‡é™åˆ¶
        max_hash_count = 20000  # å¢åŠ å“ˆå¸Œä¿ç•™æ•°é‡
        cleanup_threshold = 25000  # æ¸…ç†è§¦å‘é˜ˆå€¼
        
        if len(self.today_data['processed_lines']) > cleanup_threshold:
            # ä¿ç•™æ›´å¤šè®°å½•ä»¥æé«˜å¹‚ç­‰æ€§å¯é æ€§
            hash_list = list(self.today_data['processed_lines'])
            self.today_data['processed_lines'] = set(hash_list[-max_hash_count:])
            cleaned_count = len(hash_list) - max_hash_count
            print(f"å·²æ¸…ç† {cleaned_count} æ¡è¿‡æœŸçš„è¡Œå“ˆå¸Œè®°å½•ï¼Œä¿ç•™æœ€è¿‘ {max_hash_count} æ¡")
    
    def get_data_directory_size(self):
        """è·å–æ•°æ®ç›®å½•çš„æ€»å¤§å°"""
        total_size = 0
        file_count = 0
        
        for file_path in self.data_dir.rglob('*'):
            if file_path.is_file():
                total_size += file_path.stat().st_size
                file_count += 1
        
        return total_size, file_count
    
    def load_historical_data(self, days=7):
        """åŠ è½½å†å²æ•°æ®"""
        historical_data = []
        
        for i in range(days):
            date = datetime.now() - timedelta(days=i)
            date_str = date.strftime('%Y-%m-%d')
            data_file = self.data_dir / f"dns_data_{date_str}.json"
            
            if data_file.exists():
                with open(data_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    historical_data.append(data)
        
        return historical_data
    
    def generate_html_report(self, output_file='dnsmasq_report.html'):
        """ç”ŸæˆHTMLåˆ†ææŠ¥å‘Š"""
        # è·å–æœ€è¿‘24å°æ—¶çš„TOPåŸŸå
        top_domains_24h = self.today_data['domain_counts'].most_common(50)
        
        # è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡
        total_lookups = self.today_data['cache_hits'] + self.today_data['cache_misses']
        cache_hit_rate = (self.today_data['cache_hits'] / total_lookups * 100) if total_lookups > 0 else 0
        
        # è·å–ç¼“å­˜æœ€å¤šçš„åŸŸå
        top_cached = self.today_data['cached_domains'].most_common(10)
        top_forwarded = self.today_data['forwarded_domains'].most_common(10)
        
        # åŠ è½½7å¤©çš„å†å²æ•°æ®ï¼ˆä¸åŒ…æ‹¬ä»Šå¤©ï¼‰
        historical_data = self.load_historical_data(7)
        
        # åˆå¹¶å†å²æ•°æ®ç»Ÿè®¡ï¼ˆæ’é™¤ä»Šå¤©çš„æ•°æ®ï¼Œé¿å…é‡å¤è®¡ç®—ï¼‰
        all_time_domains = Counter()
        total_queries_7d = 0
        total_cache_hits_7d = 0
        total_cache_misses_7d = 0
        # 7å¤©ç´¯è®¡çš„å°æ—¶åˆ†å¸ƒç»Ÿè®¡
        hourly_stats_7d = defaultdict(int)
        today_str = datetime.now().strftime('%Y-%m-%d')
        
        for data in historical_data:
            # è·³è¿‡ä»Šå¤©çš„æ•°æ®ï¼Œé¿å…é‡å¤è®¡ç®—
            if data.get('date') == today_str:
                continue
                
            if 'domain_counts' in data:
                all_time_domains.update(data['domain_counts'])
                total_queries_7d += data.get('total_queries', 0)
                total_cache_hits_7d += data.get('cache_hits', 0)
                total_cache_misses_7d += data.get('cache_misses', 0)
                
                # ç´¯è®¡æ¯ä¸ªå°æ—¶çš„æŸ¥è¯¢æ•°æ®
                hourly_data = data.get('hourly_stats', {})
                for hour_str, count in hourly_data.items():
                    hourly_stats_7d[int(hour_str)] += count
        
        # æ·»åŠ å½“å¤©æ•°æ®ï¼ˆåªæ·»åŠ ä¸€æ¬¡ï¼‰
        all_time_domains.update(self.today_data['domain_counts'])
        total_queries_7d += len(self.today_data['queries'])
        total_cache_hits_7d += self.today_data['cache_hits']
        total_cache_misses_7d += self.today_data['cache_misses']
        
        # æ·»åŠ å½“å¤©çš„å°æ—¶ç»Ÿè®¡æ•°æ®
        for hour, count in self.today_data['hourly_stats'].items():
            hourly_stats_7d[hour] += count
        
        # è®¡ç®—7å¤©ç¼“å­˜å‘½ä¸­ç‡
        total_lookups_7d = total_cache_hits_7d + total_cache_misses_7d
        cache_hit_rate_7d = (total_cache_hits_7d / total_lookups_7d * 100) if total_lookups_7d > 0 else 0
        
        html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DNSmasq æ—¥å¿—åˆ†ææŠ¥å‘Š</title>
    <style>
        * {{
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }}
        
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
        }}
        
        .container {{
            max-width: 1400px;
            margin: 0 auto;
        }}
        
        .header {{
            text-align: center;
            color: white;
            margin-bottom: 30px;
            padding: 20px;
            background: rgba(255, 255, 255, 0.1);
            border-radius: 15px;
            backdrop-filter: blur(10px);
        }}
        
        .header h1 {{
            font-size: 2.5em;
            margin-bottom: 10px;
        }}
        
        .header .update-time {{
            font-size: 1.1em;
            opacity: 0.9;
        }}
        
        .stats-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }}
        
        .stat-card {{
            background: white;
            border-radius: 10px;
            padding: 20px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.1);
            transition: transform 0.3s ease;
        }}
        
        .stat-card:hover {{
            transform: translateY(-5px);
        }}
        
        .stat-card h3 {{
            color: #666;
            font-size: 0.9em;
            margin-bottom: 10px;
            text-transform: uppercase;
            letter-spacing: 1px;
        }}
        
        .stat-card .value {{
            font-size: 2.5em;
            font-weight: bold;
            color: #333;
        }}
        
        .stat-card .change {{
            font-size: 0.9em;
            color: #4CAF50;
            margin-top: 5px;
        }}
        
        .main-content {{
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin-bottom: 30px;
        }}
        
        @media (max-width: 968px) {{
            .main-content {{
                grid-template-columns: 1fr;
            }}
        }}
        
        .card {{
            background: white;
            border-radius: 10px;
            padding: 25px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.1);
        }}
        
        .card h2 {{
            color: #333;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid #f0f0f0;
        }}
        
        .domain-list {{
            max-height: 500px;
            overflow-y: auto;
        }}
        
        .domain-item {{
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 12px;
            margin-bottom: 8px;
            background: #f8f9fa;
            border-radius: 8px;
            transition: background 0.3s ease;
        }}
        
        .domain-item:hover {{
            background: #e9ecef;
        }}
        
        .domain-rank {{
            display: inline-block;
            width: 30px;
            height: 30px;
            line-height: 30px;
            text-align: center;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border-radius: 50%;
            font-weight: bold;
            margin-right: 15px;
        }}
        
        .domain-name {{
            flex: 1;
            font-weight: 500;
            color: #333;
            word-break: break-all;
        }}
        
        .domain-count {{
            background: #667eea;
            color: white;
            padding: 5px 12px;
            border-radius: 20px;
            font-weight: bold;
            font-size: 0.9em;
        }}
        
        .chart-container {{
            margin-top: 20px;
            height: 300px;
            position: relative;
        }}
        
        .hourly-chart {{
            display: flex;
            align-items: flex-end;
            height: 250px;
            padding: 10px 0;
            border-left: 2px solid #ddd;
            border-bottom: 2px solid #ddd;
        }}
        
        .hour-bar {{
            flex: 1;
            background: linear-gradient(to top, #667eea, #764ba2);
            margin: 0 2px;
            border-radius: 4px 4px 0 0;
            position: relative;
            transition: opacity 0.3s ease;
        }}
        
        .hour-bar:hover {{
            opacity: 0.8;
        }}
        
        .hour-bar::after {{
            content: attr(data-hour);
            position: absolute;
            bottom: -20px;
            left: 50%;
            transform: translateX(-50%);
            font-size: 10px;
            color: #666;
        }}
        
        .hour-bar::before {{
            content: attr(data-count);
            position: absolute;
            top: -20px;
            left: 50%;
            transform: translateX(-50%);
            font-size: 10px;
            color: #333;
            font-weight: bold;
        }}
        
        .info-grid {{
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin-top: 20px;
        }}
        
        .info-item {{
            padding: 15px;
            background: #f8f9fa;
            border-radius: 8px;
        }}
        
        .info-item h4 {{
            color: #666;
            margin-bottom: 8px;
            font-size: 0.9em;
        }}
        
        .info-item .value {{
            color: #333;
            font-size: 1.2em;
            font-weight: bold;
        }}
        
        .footer {{
            text-align: center;
            color: white;
            margin-top: 40px;
            padding: 20px;
            background: rgba(255, 255, 255, 0.1);
            border-radius: 10px;
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸŒ DNSmasq æ—¥å¿—åˆ†ææŠ¥å‘Š</h1>
            <div class="update-time">æ›´æ–°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</div>
        </div>
        
        <div class="stats-grid">
            <div class="stat-card">
                <h3>24å°æ—¶æŸ¥è¯¢æ€»æ•°</h3>
                <div class="value">{len(self.today_data['queries']):,}</div>
            </div>
            <div class="stat-card">
                <h3>24å°æ—¶ç¼“å­˜å‘½ä¸­ç‡</h3>
                <div class="value">{cache_hit_rate:.1f}%</div>
                <div class="change">å‘½ä¸­:{self.today_data['cache_hits']:,} / æœªä¸­:{self.today_data['cache_misses']:,}</div>
            </div>
            <div class="stat-card">
                <h3>ç‹¬ç«‹åŸŸåæ•°</h3>
                <div class="value">{len(self.today_data['domain_counts']):,}</div>
            </div>
            <div class="stat-card">
                <h3>æ´»è·ƒå®¢æˆ·ç«¯</h3>
                <div class="value">{len(self.today_data['client_ips']):,}</div>
            </div>
            <div class="stat-card">
                <h3>7å¤©æŸ¥è¯¢æ€»æ•°</h3>
                <div class="value">{total_queries_7d:,}</div>
            </div>
            <div class="stat-card">
                <h3>7å¤©ç¼“å­˜å‘½ä¸­ç‡</h3>
                <div class="value">{cache_hit_rate_7d:.1f}%</div>
                <div class="change">å‘½ä¸­:{total_cache_hits_7d:,} / æœªä¸­:{total_cache_misses_7d:,}</div>
            </div>
        </div>
        
        <div class="main-content">
            <div class="card">
                <h2>ğŸ“Š æœ€è¿‘24å°æ—¶é«˜é¢‘è®¿é—®åŸŸå TOP 50</h2>
                <div class="domain-list">
"""
        
        # æ·»åŠ TOPåŸŸååˆ—è¡¨
        for idx, (domain, count) in enumerate(top_domains_24h, 1):
            html_content += f"""
                    <div class="domain-item">
                        <span class="domain-rank">{idx}</span>
                        <span class="domain-name">{domain}</span>
                        <span class="domain-count">{count:,}</span>
                    </div>
"""
        
        html_content += """
                </div>
            </div>
            
            <div class="card">
                <h2>ğŸ“ˆ 7å¤©ç´¯è®¡24å°æ—¶æŸ¥è¯¢æ—¶é—´åˆ†å¸ƒ</h2>
                <div class="chart-container">
                    <div class="hourly-chart">
"""
        
        # æ·»åŠ å°æ—¶åˆ†å¸ƒå›¾ï¼ˆä½¿ç”¨7å¤©ç´¯è®¡æ•°æ®ï¼‰
        max_hourly = max(hourly_stats_7d.values()) if hourly_stats_7d else 1
        for hour in range(24):
            count = hourly_stats_7d.get(hour, 0)
            height_percent = (count / max_hourly * 100) if max_hourly > 0 else 0
            html_content += f"""
                        <div class="hour-bar" style="height: {height_percent}%;" data-hour="{hour:02d}" data-count="{count}"></div>
"""
        
        html_content += """
                    </div>
                </div>
                
                <div class="info-grid">
                    <div class="info-item">
                        <h4>æœ€æ´»è·ƒæ—¶æ®µ</h4>
                        <div class="value">{}</div>
                    </div>
                    <div class="info-item">
                        <h4>å¹³å‡æ¯å°æ—¶æŸ¥è¯¢</h4>
                        <div class="value">{}</div>
                    </div>
                    <div class="info-item">
                        <h4>æœ€å¸¸è§æŸ¥è¯¢ç±»å‹</h4>
                        <div class="value">{}</div>
                    </div>
                    <div class="info-item">
                        <h4>æœ€æ´»è·ƒå®¢æˆ·ç«¯</h4>
                        <div class="value">{}</div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="card" style="grid-column: 1 / -1;">
            <h2>ğŸ’¾ ç¼“å­˜æ€§èƒ½åˆ†æ</h2>
            <div class="main-content">
                <div>
                    <h3 style="color: #666; margin-bottom: 15px;">ğŸ”¥ ç¼“å­˜å‘½ä¸­æœ€å¤šçš„åŸŸå TOP 10</h3>
                    <div class="domain-list" style="max-height: 300px;">
""".format(
            f"{max(hourly_stats_7d, key=hourly_stats_7d.get, default=0):02d}:00" if hourly_stats_7d else "N/A",
            f"{total_queries_7d // 24:,}" if total_queries_7d else "0",
            self.today_data['query_types'].most_common(1)[0][0] if self.today_data['query_types'] else "N/A",
            self.today_data['client_ips'].most_common(1)[0][0] if self.today_data['client_ips'] else "N/A"
        )
        
        # æ·»åŠ ç¼“å­˜å‘½ä¸­TOPåŸŸå
        for idx, (domain, count) in enumerate(top_cached, 1):
            html_content += f"""
                        <div class="domain-item">
                            <span class="domain-rank">{idx}</span>
                            <span class="domain-name">{domain}</span>
                            <span class="domain-count" style="background: #4CAF50;">{count:,}</span>
                        </div>
"""
        
        html_content += """
                    </div>
                </div>
                <div>
                    <h3 style="color: #666; margin-bottom: 15px;">ğŸ”„ è½¬å‘æ¬¡æ•°æœ€å¤šçš„åŸŸå TOP 10</h3>
                    <div class="domain-list" style="max-height: 300px;">
"""
        
        # æ·»åŠ è½¬å‘TOPåŸŸå
        for idx, (domain, count) in enumerate(top_forwarded, 1):
            html_content += f"""
                        <div class="domain-item">
                            <span class="domain-rank">{idx}</span>
                            <span class="domain-name">{domain}</span>
                            <span class="domain-count" style="background: #FF9800;">{count:,}</span>
                        </div>
"""
        
        html_content += """
                    </div>
                </div>
            </div>
            
            <div class="info-grid" style="margin-top: 20px;">
                <div class="info-item">
                    <h4>ä¸Šæ¸¸DNSæœåŠ¡å™¨ä½¿ç”¨æƒ…å†µ</h4>
                    <div class="value" style="font-size: 1em;">
"""
        
        # æ·»åŠ ä¸Šæ¸¸æœåŠ¡å™¨ç»Ÿè®¡
        for server, count in self.today_data['upstream_servers'].most_common(5):
            html_content += f"                        {server}: {count:,} æ¬¡<br>"
        
        html_content += """
                    </div>
                </div>
                <div class="info-item">
                    <h4>ç¼“å­˜æ•ˆç‡æå‡</h4>
                    <div class="value">{:.1f}%</div>
                    <small style="color: #666;">å‡å°‘äº† {:.0f}% çš„ä¸Šæ¸¸æŸ¥è¯¢</small>
                </div>
            </div>
        </div>
""".format(cache_hit_rate, cache_hit_rate)
        
        # æ·»åŠ 7å¤©TOPåŸŸå
        html_content += """
        <div class="card" style="grid-column: 1 / -1;">
            <h2>ğŸ† æœ€è¿‘7å¤©é«˜é¢‘è®¿é—®åŸŸå TOP 50</h2>
            <div class="domain-list" style="column-count: 2; column-gap: 20px;">
"""
        
        for idx, (domain, count) in enumerate(all_time_domains.most_common(50), 1):
            html_content += f"""
                <div class="domain-item" style="break-inside: avoid;">
                    <span class="domain-rank">{idx}</span>
                    <span class="domain-name">{domain}</span>
                    <span class="domain-count">{count:,}</span>
                </div>
"""
        
        html_content += """
            </div>
        </div>
        
        <div class="footer">
            <p>DNSmasq Analyzer v1.0 | æ•°æ®æ¯æ—¥è‡ªåŠ¨æ›´æ–°</p>
            <p>æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {}</p>
        </div>
    </div>
</body>
</html>
""".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        print(f"HTMLæŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")

def main():
    parser = argparse.ArgumentParser(description='DNSmasqæ—¥å¿—åˆ†æå·¥å…·')
    parser.add_argument('-l', '--log', default='/var/log/dnsmasq.log', 
                       help='DNSmasqæ—¥å¿—æ–‡ä»¶è·¯å¾„ (é»˜è®¤: /var/log/dnsmasq.log)')
    parser.add_argument('-o', '--output', default='dnsmasq_report.html',
                       help='è¾“å‡ºHTMLæŠ¥å‘Šæ–‡ä»¶å (é»˜è®¤: dnsmasq_report.html)')
    parser.add_argument('-d', '--data-dir', default='./dnsmasq_data',
                       help='å†å²æ•°æ®å­˜å‚¨ç›®å½• (é»˜è®¤: ./dnsmasq_data)')
    parser.add_argument('--keep-days', type=int, default=30,
                       help='æ•°æ®æ–‡ä»¶ä¿ç•™å¤©æ•° (é»˜è®¤: 30å¤©)')
    parser.add_argument('--cleanup-only', action='store_true',
                       help='ä»…æ‰§è¡Œæ•°æ®æ¸…ç†ï¼Œä¸è¿›è¡Œæ—¥å¿—åˆ†æ')
    parser.add_argument('--include-arpa', action='store_true',
                       help='åŒ…å«.arpaåŸŸåæŸ¥è¯¢ (é»˜è®¤æ’é™¤åå‘DNSæŸ¥è¯¢)')
    
    args = parser.parse_args()
    
    print("=" * 50)
    print("DNSmasq æ—¥å¿—åˆ†æå·¥å…·")
    print("=" * 50)
    
    analyzer = DnsmasqAnalyzer(log_file=args.log, data_dir=args.data_dir, keep_days=args.keep_days, exclude_arpa=not args.include_arpa)
    
    # å¦‚æœåªæ˜¯æ¸…ç†æ¨¡å¼
    if args.cleanup_only:
        print(f"\næ­£åœ¨æ¸…ç†è¶…è¿‡ {args.keep_days} å¤©çš„æ•°æ®æ–‡ä»¶...")
        analyzer.cleanup_old_data()
        total_size, file_count = analyzer.get_data_directory_size()
        size_mb = total_size / 1024 / 1024
        print(f"æ¸…ç†å®Œæˆï¼Œå½“å‰æ•°æ®ç›®å½•: {file_count} ä¸ªæ–‡ä»¶ï¼Œæ€»å¤§å° {size_mb:.2f} MB")
        return
    
    # åˆ†ææ—¥å¿—
    print("\næ­£åœ¨åˆ†ææ—¥å¿—æ–‡ä»¶...")
    if analyzer.analyze_log():
        # ä¿å­˜æ•°æ®
        print("\næ­£åœ¨ä¿å­˜æ•°æ®...")
        analyzer.save_daily_data()
        
        # ç”ŸæˆæŠ¥å‘Š
        print("\næ­£åœ¨ç”ŸæˆHTMLæŠ¥å‘Š...")
        analyzer.generate_html_report(output_file=args.output)
        
        print("\nâœ… åˆ†æå®Œæˆ!")
        print(f"ğŸ“Š å…±åˆ†æ {len(analyzer.today_data['queries'])} æ¡æŸ¥è¯¢è®°å½•")
        print(f"ğŸ“ å†å²æ•°æ®ä¿å­˜åœ¨: {args.data_dir}")
        print(f"ğŸ“„ HTMLæŠ¥å‘Š: {args.output}")
    else:
        print("\nâŒ åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æœ‰è¯»å–æƒé™")
        sys.exit(1)

if __name__ == "__main__":
    main()