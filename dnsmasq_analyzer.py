#!/usr/bin/env python3

import re
import sqlite3
import json
import os
from datetime import datetime, timedelta
from collections import Counter, defaultdict
from pathlib import Path
import argparse
import sys
import requests
import hashlib

class DnsmasqAnalyzer:
    def __init__(self, log_file='/var/log/dnsmasq.log', data_dir='./dnsmasq_data', keep_days=30, exclude_arpa=True):
        self.log_file = log_file
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        self.keep_days = keep_days  # ä¿ç•™å¤©æ•°ï¼Œé»˜è®¤30å¤©
        self.exclude_arpa = exclude_arpa  # æ˜¯å¦æ’é™¤.arpaåŸŸå
        
        # SQLite æ•°æ®åº“é…ç½®
        self.db_file = self.data_dir / 'dnsmasq_analysis.db'
        self.init_database()
        
        # DeepSeek AIé…ç½®
        self.deepseek_api_key = self.load_deepseek_config()
        self.deepseek_api_base = "https://api.deepseek.com/v1"
        
        # çŠ¶æ€æ–‡ä»¶ï¼Œè®°å½•ä¸Šæ¬¡å¤„ç†çš„ä½ç½®
        self.state_file = self.data_dir / '.last_processed_state.json'
        self.last_processed_time = None
        self.load_state()
        
        # æ­£åˆ™è¡¨è¾¾å¼è§£ædnsmasqæ—¥å¿—
        # æ ¼å¼: Dec 30 12:30:45 dnsmasq[12345]: query[A] domain.com from 192.168.1.1
        self.log_pattern = re.compile(
            r'(\w+\s+\d+\s+\d+:\d+:\d+).*?query\[(\w+)\]\s+([^\s]+)\s+from\s+([^\s]+)'
        )
        
        # ç¼“å­˜å‘½ä¸­çš„æ­£åˆ™è¡¨è¾¾å¼
        # æ ¼å¼: Dec 30 12:30:45 dnsmasq[12345]: cached domain.com is 1.2.3.4
        self.cache_pattern = re.compile(
            r'(\w+\s+\d+\s+\d+:\d+:\d+).*?cached\s+([^\s]+)\s+'
        )
        
        # è½¬å‘æŸ¥è¯¢çš„æ­£åˆ™è¡¨è¾¾å¼
        # æ ¼å¼: Dec 30 12:30:45 dnsmasq[12345]: forwarded domain.com to 8.8.8.8
        # æˆ–: Dec 30 12:30:45 dnsmasq[12345]: forwarded domain.com to 8.8.8.8#53
        # æˆ–: Dec 30 12:30:45 dnsmasq[12345]: forwarded domain.com to 192.168.1.1#5353
        self.forward_pattern = re.compile(
            r'(\w+\s+\d+\s+\d+:\d+:\d+).*?forwarded\s+([^\s]+)\s+to\s+([^\s]+(?:#\d+)?)'
        )
        
    
    def init_database(self):
        """åˆå§‹åŒ– SQLite æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_file)
        cursor = conn.cursor()
        
        # åˆ›å»º DNS æŸ¥è¯¢è®°å½•è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS dns_queries (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                line_hash TEXT UNIQUE NOT NULL,
                timestamp DATETIME NOT NULL,
                query_type TEXT NOT NULL,
                domain TEXT NOT NULL,
                client_ip TEXT NOT NULL,
                record_type TEXT NOT NULL,
                date_only DATE NOT NULL,
                hour INTEGER NOT NULL,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # åˆ›å»ºç¼“å­˜å‘½ä¸­è®°å½•è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS cache_hits (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                line_hash TEXT UNIQUE NOT NULL,
                timestamp DATETIME NOT NULL,
                domain TEXT NOT NULL,
                date_only DATE NOT NULL,
                hour INTEGER NOT NULL,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # åˆ›å»º DNS è½¬å‘è®°å½•è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS dns_forwards (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                line_hash TEXT UNIQUE NOT NULL,
                timestamp DATETIME NOT NULL,
                domain TEXT NOT NULL,
                upstream_server TEXT NOT NULL,
                date_only DATE NOT NULL,
                hour INTEGER NOT NULL,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # åˆ›å»ºç´¢å¼•æé«˜æŸ¥è¯¢æ€§èƒ½
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_queries_date ON dns_queries(date_only)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_queries_domain ON dns_queries(domain)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_queries_client ON dns_queries(client_ip)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_queries_hour ON dns_queries(hour)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_queries_timestamp ON dns_queries(timestamp)')
        
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_cache_date ON cache_hits(date_only)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_cache_domain ON cache_hits(domain)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_cache_hour ON cache_hits(hour)')
        
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_forwards_date ON dns_forwards(date_only)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_forwards_domain ON dns_forwards(domain)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_forwards_upstream ON dns_forwards(upstream_server)')
        
        conn.commit()
        conn.close()
        
    def get_db_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥"""
        return sqlite3.connect(self.db_file)
    
    def get_statistics_from_db(self, date_filter=None):
        """ä»æ•°æ®åº“è·å–ç»Ÿè®¡æ•°æ®"""
        conn = self.get_db_connection()
        cursor = conn.cursor()
        
        if date_filter is None:
            date_filter = datetime.now().strftime('%Y-%m-%d')
        
        try:
            # æŸ¥è¯¢æ€»æ•°
            cursor.execute('SELECT COUNT(*) FROM dns_queries WHERE date_only = ?', (date_filter,))
            total_queries = cursor.fetchone()[0]
            
            # ç¼“å­˜å‘½ä¸­æ•°
            cursor.execute('SELECT COUNT(*) FROM cache_hits WHERE date_only = ?', (date_filter,))
            cache_hits = cursor.fetchone()[0]
            
            # ç¼“å­˜æœªå‘½ä¸­æ•°ï¼ˆè½¬å‘æ•°ï¼‰
            cursor.execute('SELECT COUNT(*) FROM dns_forwards WHERE date_only = ?', (date_filter,))
            cache_misses = cursor.fetchone()[0]
            
            # è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡
            total_lookups = cache_hits + cache_misses
            cache_hit_rate = (cache_hits / total_lookups * 100) if total_lookups > 0 else 0
            
            return {
                'total_queries': total_queries,
                'cache_hits': cache_hits,
                'cache_misses': cache_misses,
                'cache_hit_rate': cache_hit_rate,
                'total_lookups': total_lookups
            }
        finally:
            conn.close()
    
    def get_24h_statistics_from_db(self):
        """è·å–è¿‡å»24å°æ—¶çš„çœŸå®ç»Ÿè®¡æ•°æ®"""
        conn = self.get_db_connection()
        cursor = conn.cursor()
        
        # è®¡ç®—24å°æ—¶å‰çš„æ—¶é—´
        now = datetime.now()
        hours_24_ago = now - timedelta(hours=24)
        
        try:
            # è¿‡å»24å°æ—¶çš„æŸ¥è¯¢æ€»æ•°
            cursor.execute('''
                SELECT COUNT(*) FROM dns_queries 
                WHERE timestamp >= ?
            ''', (hours_24_ago,))
            total_queries = cursor.fetchone()[0]
            
            # è¿‡å»24å°æ—¶çš„ç¼“å­˜å‘½ä¸­æ•°
            cursor.execute('''
                SELECT COUNT(*) FROM cache_hits 
                WHERE timestamp >= ?
            ''', (hours_24_ago,))
            cache_hits = cursor.fetchone()[0]
            
            # è¿‡å»24å°æ—¶çš„ç¼“å­˜æœªå‘½ä¸­æ•°
            cursor.execute('''
                SELECT COUNT(*) FROM dns_forwards 
                WHERE timestamp >= ?
            ''', (hours_24_ago,))
            cache_misses = cursor.fetchone()[0]
            
            # è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡
            total_lookups = cache_hits + cache_misses
            cache_hit_rate = (cache_hits / total_lookups * 100) if total_lookups > 0 else 0
            
            return {
                'total_queries': total_queries,
                'cache_hits': cache_hits,
                'cache_misses': cache_misses,
                'cache_hit_rate': cache_hit_rate,
                'total_lookups': total_lookups
            }
        finally:
            conn.close()
    
    def get_top_domains_from_db(self, date_filter=None, limit=50):
        """ä»æ•°æ®åº“è·å–é«˜é¢‘åŸŸå"""
        conn = self.get_db_connection()
        cursor = conn.cursor()
        
        if date_filter is None:
            date_filter = datetime.now().strftime('%Y-%m-%d')
        
        try:
            cursor.execute('''
                SELECT domain, COUNT(*) as count
                FROM dns_queries 
                WHERE date_only = ?
                GROUP BY domain 
                ORDER BY count DESC 
                LIMIT ?
            ''', (date_filter, limit))
            
            return cursor.fetchall()
        finally:
            conn.close()
    
    def get_top_domains_24h_from_db(self, limit=50):
        """ä»æ•°æ®åº“è·å–è¿‡å»24å°æ—¶çš„é«˜é¢‘åŸŸå"""
        conn = self.get_db_connection()
        cursor = conn.cursor()
        
        # è®¡ç®—24å°æ—¶å‰çš„æ—¶é—´
        now = datetime.now()
        hours_24_ago = now - timedelta(hours=24)
        
        try:
            cursor.execute('''
                SELECT domain, COUNT(*) as count
                FROM dns_queries 
                WHERE timestamp >= ?
                GROUP BY domain 
                ORDER BY count DESC 
                LIMIT ?
            ''', (hours_24_ago, limit))
            
            return cursor.fetchall()
        finally:
            conn.close()
    
    def get_hourly_stats_from_db(self, days=1):
        """ä»æ•°æ®åº“è·å–æŒ‰å°æ—¶ç»Ÿè®¡çš„æ•°æ®"""
        conn = self.get_db_connection()
        cursor = conn.cursor()
        
        end_date = datetime.now().strftime('%Y-%m-%d')
        start_date = (datetime.now() - timedelta(days=days-1)).strftime('%Y-%m-%d')
        
        try:
            cursor.execute('''
                SELECT hour, COUNT(*) as count
                FROM dns_queries 
                WHERE date_only BETWEEN ? AND ?
                GROUP BY hour 
                ORDER BY hour
            ''', (start_date, end_date))
            
            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            hourly_stats = {}
            for hour, count in cursor.fetchall():
                hourly_stats[hour] = count
            
            return hourly_stats
        finally:
            conn.close()
    
    def get_client_stats_from_db(self, date_filter=None, limit=6):
        """ä»æ•°æ®åº“è·å–å®¢æˆ·ç«¯ç»Ÿè®¡æ•°æ®"""
        conn = self.get_db_connection()
        cursor = conn.cursor()
        
        if date_filter is None:
            date_filter = datetime.now().strftime('%Y-%m-%d')
        
        try:
            # è·å–æœ€æ´»è·ƒçš„å®¢æˆ·ç«¯
            cursor.execute('''
                SELECT client_ip, COUNT(*) as count
                FROM dns_queries 
                WHERE date_only = ?
                GROUP BY client_ip 
                ORDER BY count DESC 
                LIMIT ?
            ''', (date_filter, limit))
            
            top_clients = cursor.fetchall()
            
            # è·å–æ¯ä¸ªå®¢æˆ·ç«¯çš„é«˜é¢‘åŸŸå
            result = []
            for client_ip, total_queries in top_clients:
                cursor.execute('''
                    SELECT domain, COUNT(*) as count
                    FROM dns_queries 
                    WHERE date_only = ? AND client_ip = ?
                    GROUP BY domain 
                    ORDER BY count DESC 
                    LIMIT 10
                ''', (date_filter, client_ip))
                
                top_domains = cursor.fetchall()
                result.append({
                    'client_ip': client_ip,
                    'total_queries': total_queries,
                    'top_domains': top_domains
                })
            
            return result
        finally:
            conn.close()
    
    def get_client_stats_24h_from_db(self, limit=6):
        """ä»æ•°æ®åº“è·å–è¿‡å»24å°æ—¶çš„å®¢æˆ·ç«¯ç»Ÿè®¡æ•°æ®"""
        conn = self.get_db_connection()
        cursor = conn.cursor()
        
        # è®¡ç®—24å°æ—¶å‰çš„æ—¶é—´
        now = datetime.now()
        hours_24_ago = now - timedelta(hours=24)
        
        try:
            # è·å–æœ€æ´»è·ƒçš„å®¢æˆ·ç«¯
            cursor.execute('''
                SELECT client_ip, COUNT(*) as count
                FROM dns_queries 
                WHERE timestamp >= ?
                GROUP BY client_ip 
                ORDER BY count DESC 
                LIMIT ?
            ''', (hours_24_ago, limit))
            
            top_clients = cursor.fetchall()
            
            # è·å–æ¯ä¸ªå®¢æˆ·ç«¯çš„é«˜é¢‘åŸŸå
            result = []
            for client_ip, total_queries in top_clients:
                cursor.execute('''
                    SELECT domain, COUNT(*) as count
                    FROM dns_queries 
                    WHERE timestamp >= ? AND client_ip = ?
                    GROUP BY domain 
                    ORDER BY count DESC 
                    LIMIT 10
                ''', (hours_24_ago, client_ip))
                
                top_domains = cursor.fetchall()
                result.append({
                    'client_ip': client_ip,
                    'total_queries': total_queries,
                    'top_domains': top_domains
                })
            
            return result
        finally:
            conn.close()
    
    def get_cache_stats_from_db(self, date_filter=None, limit=10):
        """ä»æ•°æ®åº“è·å–ç¼“å­˜ç»Ÿè®¡æ•°æ®"""
        conn = self.get_db_connection()
        cursor = conn.cursor()
        
        if date_filter is None:
            date_filter = datetime.now().strftime('%Y-%m-%d')
        
        try:
            # è·å–ç¼“å­˜å‘½ä¸­æœ€å¤šçš„åŸŸå
            cursor.execute('''
                SELECT domain, COUNT(*) as count
                FROM cache_hits 
                WHERE date_only = ?
                GROUP BY domain 
                ORDER BY count DESC 
                LIMIT ?
            ''', (date_filter, limit))
            top_cached = cursor.fetchall()
            
            # è·å–è½¬å‘æœ€å¤šçš„åŸŸå
            cursor.execute('''
                SELECT domain, COUNT(*) as count
                FROM dns_forwards 
                WHERE date_only = ?
                GROUP BY domain 
                ORDER BY count DESC 
                LIMIT ?
            ''', (date_filter, limit))
            top_forwarded = cursor.fetchall()
            
            # è·å–ä¸Šæ¸¸æœåŠ¡å™¨ç»Ÿè®¡
            cursor.execute('''
                SELECT upstream_server, COUNT(*) as count
                FROM dns_forwards 
                WHERE date_only = ?
                GROUP BY upstream_server 
                ORDER BY count DESC 
                LIMIT ?
            ''', (date_filter, limit))
            upstream_servers = cursor.fetchall()
            
            return {
                'top_cached': top_cached,
                'top_forwarded': top_forwarded,
                'upstream_servers': upstream_servers
            }
        finally:
            conn.close()
    
    def get_multi_day_stats_from_db(self, days=7):
        """ä»æ•°æ®åº“è·å–å¤šå¤©ç»Ÿè®¡æ•°æ®"""
        conn = self.get_db_connection()
        cursor = conn.cursor()
        
        end_date = datetime.now().strftime('%Y-%m-%d')
        start_date = (datetime.now() - timedelta(days=days-1)).strftime('%Y-%m-%d')
        
        try:
            # è·å–å¤šå¤©é«˜é¢‘åŸŸå
            cursor.execute('''
                SELECT domain, COUNT(*) as count
                FROM dns_queries 
                WHERE date_only BETWEEN ? AND ?
                GROUP BY domain 
                ORDER BY count DESC 
                LIMIT 50
            ''', (start_date, end_date))
            top_domains = cursor.fetchall()
            
            # è·å–æ€»ç»Ÿè®¡
            cursor.execute('''
                SELECT COUNT(*) FROM dns_queries 
                WHERE date_only BETWEEN ? AND ?
            ''', (start_date, end_date))
            total_queries = cursor.fetchone()[0]
            
            cursor.execute('''
                SELECT COUNT(*) FROM cache_hits 
                WHERE date_only BETWEEN ? AND ?
            ''', (start_date, end_date))
            total_cache_hits = cursor.fetchone()[0]
            
            cursor.execute('''
                SELECT COUNT(*) FROM dns_forwards 
                WHERE date_only BETWEEN ? AND ?
            ''', (start_date, end_date))
            total_cache_misses = cursor.fetchone()[0]
            
            # è·å–æŒ‰å°æ—¶çš„ç»Ÿè®¡
            hourly_stats = self.get_hourly_stats_from_db(days)
            
            total_lookups = total_cache_hits + total_cache_misses
            cache_hit_rate = (total_cache_hits / total_lookups * 100) if total_lookups > 0 else 0
            
            return {
                'top_domains': top_domains,
                'total_queries': total_queries,
                'total_cache_hits': total_cache_hits,
                'total_cache_misses': total_cache_misses,
                'cache_hit_rate': cache_hit_rate,
                'hourly_stats': hourly_stats
            }
        finally:
            conn.close()
        
    def load_deepseek_config(self):
        """åŠ è½½DeepSeek APIé…ç½®"""
        # æŒ‰ä¼˜å…ˆçº§ä¾æ¬¡æ£€æŸ¥é…ç½®æº
        
        # 1. ç¯å¢ƒå˜é‡
        api_key = os.getenv('DEEPSEEK_API_KEY')
        if api_key:
            return api_key
        
        # 2. é…ç½®æ–‡ä»¶
        config_paths = [
            self.data_dir / 'deepseek_config.json',
            Path.home() / '.config' / 'dnsmasq_analyzer' / 'deepseek_config.json',
            Path('/etc/dnsmasq_analyzer/deepseek_config.json')
        ]
        
        for config_path in config_paths:
            if config_path.exists():
                try:
                    with open(config_path, 'r') as f:
                        config = json.load(f)
                        if 'api_key' in config:
                            return config['api_key']
                except Exception as e:
                    print(f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥ {config_path}: {e}")
        
        return None
    
    def setup_deepseek_config(self, api_key=None):
        """è®¾ç½®DeepSeek APIé…ç½®"""
        if api_key is None:
            # äº¤äº’å¼è®¾ç½®
            print("\n=== DeepSeek AI é…ç½®è®¾ç½® ===")
            print("è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è·å–APIå¯†é’¥ï¼š")
            print("1. è®¿é—® https://platform.deepseek.com/")
            print("2. æ³¨å†Œå¹¶ç™»å½•è´¦æˆ·")
            print("3. åœ¨æ§åˆ¶å°åˆ›å»ºAPIå¯†é’¥")
            print("4. å¤åˆ¶APIå¯†é’¥å¹¶ç²˜è´´åˆ°ä¸‹æ–¹")
            print()
            
            api_key = input("è¯·è¾“å…¥æ‚¨çš„DeepSeek APIå¯†é’¥: ").strip()
        
        if not api_key:
            print("âŒ APIå¯†é’¥ä¸èƒ½ä¸ºç©º")
            return False
        
        # éªŒè¯APIå¯†é’¥æ ¼å¼
        if not api_key.startswith('sk-'):
            print("âš ï¸ è­¦å‘Šï¼šAPIå¯†é’¥æ ¼å¼å¯èƒ½ä¸æ­£ç¡®ï¼Œé€šå¸¸ä»¥ 'sk-' å¼€å¤´")
        
        # ä¿å­˜é…ç½®
        config_dir = self.data_dir
        config_file = config_dir / 'deepseek_config.json'
        
        config = {
            'api_key': api_key,
            'created_at': datetime.now().isoformat(),
            'api_base': self.deepseek_api_base
        }
        
        try:
            with open(config_file, 'w') as f:
                json.dump(config, f, indent=2)
            
            # è®¾ç½®æ–‡ä»¶æƒé™ä¸ºåªæœ‰ç”¨æˆ·å¯è¯»å†™
            os.chmod(config_file, 0o600)
            
            print(f"âœ… é…ç½®å·²ä¿å­˜åˆ°: {config_file}")
            print("ğŸ’¡ æç¤ºï¼šæ‚¨ä¹Ÿå¯ä»¥è®¾ç½®ç¯å¢ƒå˜é‡ DEEPSEEK_API_KEY æ¥é…ç½®APIå¯†é’¥")
            
            self.deepseek_api_key = api_key
            return True
            
        except Exception as e:
            print(f"âŒ ä¿å­˜é…ç½®å¤±è´¥: {e}")
            return False

    def is_arpa_domain(self, domain):
        """æ£€æŸ¥æ˜¯å¦ä¸º.arpaåŸŸåï¼ˆåå‘DNSæŸ¥è¯¢ï¼‰"""
        return domain.endswith('.arpa')
    
    def is_within_analysis_window(self, timestamp):
        """æ™ºèƒ½çš„æ—¶é—´çª—å£æ£€æŸ¥ï¼Œå¤„ç†è¾¹ç•Œæ¡ä»¶"""
        now = datetime.now()
        today_start = now.replace(hour=0, minute=0, second=0, microsecond=0)
        tomorrow_start = today_start + timedelta(days=1)
        
        # æ£€æŸ¥æ˜¯å¦åœ¨å½“å¤©èŒƒå›´å†…ï¼ˆä»ä»Šå¤©00:00åˆ°ç°åœ¨ï¼‰
        if today_start <= timestamp <= now:
            return True
        
        # å¤„ç†è·¨å¤©æƒ…å†µï¼šå¦‚æœå½“å‰æ—¶é—´æ˜¯å‡Œæ™¨ï¼Œå¯èƒ½éœ€è¦åŒ…å«æ˜¨å¤©æ™šä¸Šçš„æ—¥å¿—
        if now.hour < 2:  # å‡Œæ™¨2ç‚¹å‰
            yesterday_22 = today_start - timedelta(hours=2)  # æ˜¨å¤©22ç‚¹
            if yesterday_22 <= timestamp < today_start:
                return True
        
        # æ‰©å±•çª—å£ï¼šåŒ…å«æœ€è¿‘7å¤©çš„æ•°æ®ï¼ˆç”¨äºå†å²æ—¥å¿—åˆ†æï¼‰
        week_ago = today_start - timedelta(days=7)
        if week_ago <= timestamp < today_start:
            return True
        
        # å¤„ç†æ—¶é—´æˆ³ç•¥å¾®è¶…å‰çš„æƒ…å†µï¼ˆå¯èƒ½çš„ç³»ç»Ÿæ—¶é—´å·®å¼‚ï¼‰ï¼Œä½†ä¸è¶…è¿‡æ˜å¤©
        if now < timestamp <= min(now + timedelta(minutes=10), tomorrow_start):
            return True
            
        return False
        
    def parse_timestamp(self, timestamp_str):
        """å¥å£®çš„æ—¶é—´æˆ³è§£ææ–¹æ³•"""
        current_year = datetime.now().year
        now = datetime.now()
        
        # å°è¯•å¤šç§æ—¶é—´æˆ³æ ¼å¼
        timestamp_formats = [
            # ä¸åŒ…å«å¹´ä»½çš„æ ¼å¼ï¼ˆæœ€å¸¸è§ï¼‰
            "%b %d %H:%M:%S",                     # Aug 16 05:00:06
            "%B %d %H:%M:%S",                     # August 16 05:00:06
            # åŒ…å«å¹´ä»½çš„æ ¼å¼
            "%Y %b %d %H:%M:%S",                  # 2025 Aug 16 05:00:06
            "%Y %B %d %H:%M:%S",                  # 2025 August 16 05:00:06
            "%Y-%m-%d %H:%M:%S",                  # 2025-08-16 05:00:06
        ]
        
        for fmt in timestamp_formats:
            try:
                # æ ¹æ®æ—¶é—´æˆ³æ˜¯å¦åŒ…å«å¹´ä»½æ¥å†³å®šå¤„ç†æ–¹å¼
                if timestamp_str.strip().startswith(('19', '20')):  # åŒ…å«å¹´ä»½
                    parsed_time = datetime.strptime(timestamp_str, fmt)
                else:  # ä¸åŒ…å«å¹´ä»½ï¼Œéœ€è¦æ·»åŠ å½“å‰å¹´ä»½
                    if "%Y" in fmt:
                        # è·³è¿‡åŒ…å«å¹´ä»½çš„æ ¼å¼ï¼Œå› ä¸ºæ—¶é—´æˆ³ä¸åŒ…å«å¹´ä»½
                        continue
                    # è§£æä¸å«å¹´ä»½çš„æ—¶é—´æˆ³
                    parsed_time = datetime.strptime(timestamp_str, fmt)
                    # æ‰‹åŠ¨æ·»åŠ å¹´ä»½
                    parsed_time = parsed_time.replace(year=current_year)
                
                # æ£€æŸ¥è§£æçš„æ—¶é—´æ˜¯å¦åˆç†ï¼ˆä¸è¶…è¿‡å½“å‰æ—¶é—´å¤ªè¿œï¼‰
                time_diff = abs((now - parsed_time).total_seconds())
                if time_diff > 366 * 24 * 3600:  # è¶…è¿‡ä¸€å¹´
                    # å°è¯•ä½¿ç”¨å‰ä¸€å¹´
                    try:
                        adjusted_time = parsed_time.replace(year=current_year - 1)
                        if abs((now - adjusted_time).total_seconds()) <= 366 * 24 * 3600:
                            return adjusted_time
                    except ValueError:
                        pass
                    # å¦‚æœè°ƒæ•´å¹´ä»½åè¿˜æ˜¯ä¸åˆç†ï¼Œç»§ç»­å°è¯•å…¶ä»–æ ¼å¼
                    continue
                
                return parsed_time
            except ValueError:
                continue
        
        # æœ€åçš„fallbackï¼šä½¿ç”¨å½“å‰æ—¶é—´ï¼Œä½†è®°å½•è­¦å‘Š
        print(f"è­¦å‘Šï¼šæ— æ³•è§£ææ—¶é—´æˆ³ '{timestamp_str}'ï¼Œä½¿ç”¨å½“å‰æ—¶é—´")
        return now

    def parse_log_line(self, line):
        """è§£æå•è¡Œæ—¥å¿—"""
        # æ£€æŸ¥æ˜¯å¦æ˜¯æŸ¥è¯¢è®°å½•
        match = self.log_pattern.search(line)
        if match:
            timestamp_str, query_type, domain, client_ip = match.groups()
            timestamp = self.parse_timestamp(timestamp_str)
            
            return {
                'type': 'query',
                'timestamp': timestamp,
                'query_type': query_type,
                'domain': domain.lower(),
                'client_ip': client_ip,
                'hour': timestamp.hour
            }
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç¼“å­˜å‘½ä¸­è®°å½•
        cache_match = self.cache_pattern.search(line)
        if cache_match:
            timestamp_str, domain = cache_match.groups()
            timestamp = self.parse_timestamp(timestamp_str)
            
            return {
                'type': 'cache_hit',
                'timestamp': timestamp,
                'domain': domain.lower(),
                'hour': timestamp.hour
            }
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯è½¬å‘è®°å½•
        forward_match = self.forward_pattern.search(line)
        if forward_match:
            timestamp_str, domain, upstream = forward_match.groups()
            timestamp = self.parse_timestamp(timestamp_str)
            
            return {
                'type': 'forward',
                'timestamp': timestamp,
                'domain': domain.lower(),
                'upstream': upstream,
                'hour': timestamp.hour
            }
        
        return None
    
    def load_state(self):
        """åŠ è½½ä¸Šæ¬¡å¤„ç†çŠ¶æ€"""
        if self.state_file.exists():
            try:
                with open(self.state_file, 'r') as f:
                    state = json.load(f)
                    self.last_processed_time = datetime.fromisoformat(state.get('last_processed_time', ''))
                    print(f"ä¸Šæ¬¡å¤„ç†æ—¶é—´: {self.last_processed_time}")
            except Exception as e:
                print(f"åŠ è½½çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}")
                self.last_processed_time = None
    
    def save_state(self):
        """ä¿å­˜å¤„ç†çŠ¶æ€"""
        state = {
            'last_processed_time': datetime.now().isoformat(),
            'log_file': self.log_file
        }
        with open(self.state_file, 'w') as f:
            json.dump(state, f, indent=2)
    
    def get_line_hash(self, line):
        """ç”Ÿæˆæ—¥å¿—è¡Œçš„å”¯ä¸€æ ‡è¯†ç¬¦ï¼ˆåŒ…å«æ—¶é—´æˆ³å’Œæ–‡ä»¶ä½ç½®ä¿¡æ¯å¢å¼ºå”¯ä¸€æ€§ï¼‰"""
        # æå–æ—¶é—´æˆ³ä¿¡æ¯ä»¥å¢å¼ºå”¯ä¸€æ€§
        timestamp_info = ""
        parsed_data = self.parse_log_line(line)
        if parsed_data and 'timestamp' in parsed_data:
            timestamp_info = parsed_data['timestamp'].isoformat()
        
        # ç»“åˆåŸå§‹è¡Œå†…å®¹ã€æ—¶é—´æˆ³å’Œè¡Œé•¿åº¦åˆ›å»ºæ›´å¼ºçš„å”¯ä¸€æ ‡è¯†
        unique_string = f"{line.strip()}|{timestamp_info}|{len(line)}"
        return hashlib.sha256(unique_string.encode()).hexdigest()[:32]  # ä½¿ç”¨SHA256å¹¶æˆªå–å‰32ä½
    
    def markdown_to_html(self, markdown_text):
        """å°†markdownæ–‡æœ¬è½¬æ¢ä¸ºHTMLæ ¼å¼"""
        if not markdown_text:
            return ""
        
        # è½¬ä¹‰HTMLç‰¹æ®Šå­—ç¬¦ï¼ˆä½†ä¿ç•™æ¢è¡Œç¬¦ç”¨äºåç»­å¤„ç†ï¼‰
        html_text = markdown_text.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')
        
        # å…ˆå¤„ç†ä»£ç å—å’Œè¡Œå†…ä»£ç ï¼Œé¿å…è¢«å…¶ä»–è§„åˆ™å¹²æ‰°
        # å¤„ç†ä»£ç å— ```
        html_text = re.sub(r'```(\w+)?\n(.*?)\n```', 
                          r'<pre><code>\2</code></pre>', 
                          html_text, flags=re.DOTALL)
        
        # å¤„ç†è¡Œå†…ä»£ç  `code` (é¿å…ä¸ç²—ä½“æ–œä½“å†²çª)
        html_text = re.sub(r'`([^`]+)`', r'<code>\1</code>', html_text)
        
        # å¤„ç†ç²—ä½“ **text** æˆ– __text__
        html_text = re.sub(r'\*\*(.*?)\*\*', r'<strong>\1</strong>', html_text)
        html_text = re.sub(r'__(.*?)__', r'<strong>\1</strong>', html_text)
        
        # å¤„ç†æ–œä½“ *text* æˆ– _text_ (æ³¨æ„é¿å…ä¸åˆ—è¡¨å†²çª)
        html_text = re.sub(r'(?<!\*)\*([^*\n]+)\*(?!\*)', r'<em>\1</em>', html_text)
        html_text = re.sub(r'(?<!_)_([^_\n]+)_(?!_)', r'<em>\1</em>', html_text)
        
        # æŒ‰æ®µè½åˆ†å‰²å¤„ç†
        paragraphs = html_text.split('\n\n')
        processed_paragraphs = []
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
                
            lines = para.split('\n')
            processed_lines = []
            in_list = False
            list_items = []
            
            current_list_type = None  # 'ul' for unordered, 'ol' for ordered
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                    
                # å¤„ç†æ ‡é¢˜
                if line.startswith('####'):
                    if in_list:
                        list_tag = current_list_type or 'ul'
                        processed_lines.append(f'<{list_tag}>{"".join(list_items)}</{list_tag}>')
                        list_items = []
                        in_list = False
                        current_list_type = None
                    processed_lines.append(f'<h4>{line[4:].strip()}</h4>')
                elif line.startswith('###'):
                    if in_list:
                        list_tag = current_list_type or 'ul'
                        processed_lines.append(f'<{list_tag}>{"".join(list_items)}</{list_tag}>')
                        list_items = []
                        in_list = False
                        current_list_type = None
                    processed_lines.append(f'<h3>{line[3:].strip()}</h3>')
                elif line.startswith('##'):
                    if in_list:
                        list_tag = current_list_type or 'ul'
                        processed_lines.append(f'<{list_tag}>{"".join(list_items)}</{list_tag}>')
                        list_items = []
                        in_list = False
                        current_list_type = None
                    processed_lines.append(f'<h2>{line[2:].strip()}</h2>')
                elif line.startswith('#'):
                    if in_list:
                        list_tag = current_list_type or 'ul'
                        processed_lines.append(f'<{list_tag}>{"".join(list_items)}</{list_tag}>')
                        list_items = []
                        in_list = False
                        current_list_type = None
                    processed_lines.append(f'<h1>{line[1:].strip()}</h1>')
                
                # å¤„ç†æ— åºåˆ—è¡¨é¡¹
                elif line.startswith('- ') or line.startswith('* '):
                    if in_list and current_list_type == 'ol':
                        # å¦‚æœå‰é¢æ˜¯æœ‰åºåˆ—è¡¨ï¼Œå…ˆå…³é—­
                        processed_lines.append(f'<ol>{"".join(list_items)}</ol>')
                        list_items = []
                    content = line[2:].strip()
                    list_items.append(f'<li>{content}</li>')
                    in_list = True
                    current_list_type = 'ul'
                
                # å¤„ç†æœ‰åºåˆ—è¡¨é¡¹ - ä¿®å¤é—®é¢˜ï¼šæ”¯æŒæ›´çµæ´»çš„æœ‰åºåˆ—è¡¨æ ¼å¼
                elif re.match(r'^\d+[\.\)]\s+', line):
                    if in_list and current_list_type == 'ul':
                        # å¦‚æœå‰é¢æ˜¯æ— åºåˆ—è¡¨ï¼Œå…ˆå…³é—­
                        processed_lines.append(f'<ul>{"".join(list_items)}</ul>')
                        list_items = []
                    # ç§»é™¤æ•°å­—å’Œæ ‡ç‚¹ç¬¦å·ï¼ˆç‚¹å·æˆ–æ‹¬å·ï¼‰
                    content = re.sub(r'^\d+[\.\)]\s+', '', line)
                    list_items.append(f'<li>{content}</li>')
                    in_list = True
                    current_list_type = 'ol'
                
                # æ™®é€šæ–‡æœ¬è¡Œ
                else:
                    if in_list:
                        list_tag = current_list_type or 'ul'
                        processed_lines.append(f'<{list_tag}>{"".join(list_items)}</{list_tag}>')
                        list_items = []
                        in_list = False
                        current_list_type = None
                    processed_lines.append(line)
            
            # å¤„ç†æ®µè½ç»“æŸæ—¶çš„åˆ—è¡¨
            if in_list and list_items:
                list_tag = current_list_type or 'ul'
                processed_lines.append(f'<{list_tag}>{"".join(list_items)}</{list_tag}>')
            
            # å°†éHTMLæ ‡ç­¾çš„è¿ç»­è¡ŒåŒ…è£…ä¸ºæ®µè½
            if processed_lines:
                para_content = '\n'.join(processed_lines)
                
                # åˆ†ç¦»HTMLæ ‡ç­¾å’Œæ™®é€šæ–‡æœ¬
                html_elements = []
                current_text = []
                
                for line in processed_lines:
                    if re.match(r'^\s*<(?:h[1-6]|ul|ol|pre)', line):
                        # å¦‚æœä¹‹å‰æœ‰æ™®é€šæ–‡æœ¬ï¼ŒåŒ…è£…ä¸ºæ®µè½
                        if current_text:
                            text_content = ' '.join(current_text)
                            html_elements.append(f'<p>{text_content}</p>')
                            current_text = []
                        html_elements.append(line)
                    else:
                        current_text.append(line)
                
                # å¤„ç†å‰©ä½™çš„æ™®é€šæ–‡æœ¬
                if current_text:
                    text_content = ' '.join(current_text)
                    html_elements.append(f'<p>{text_content}</p>')
                
                processed_paragraphs.extend(html_elements)
        
        # ç»„åˆç»“æœ
        result = '\n'.join(processed_paragraphs)
        
        # æ¸…ç†å¤šä½™çš„ç©ºç™½å’Œæ¢è¡Œ
        result = re.sub(r'\n\s*\n', '\n', result)
        result = result.strip()
        
        return result

    def call_deepseek_api(self, prompt, max_tokens=1000):
        """è°ƒç”¨DeepSeek APIè¿›è¡ŒAIåˆ†æ"""
        if not self.deepseek_api_key:
            return None
        
        headers = {
            'Authorization': f'Bearer {self.deepseek_api_key}',
            'Content-Type': 'application/json'
        }
        
        data = {
            'model': 'deepseek-chat',
            'messages': [
                {
                    'role': 'system',
                    'content': 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç½‘ç»œå®‰å…¨åˆ†æå¸ˆï¼Œä¸“é—¨åˆ†æDNSæŸ¥è¯¢æ—¥å¿—ï¼Œè¯†åˆ«å¼‚å¸¸è¡Œä¸ºå’Œæ½œåœ¨å¨èƒã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œè¯­è¨€ç®€æ´æ˜äº†ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è¦æ±‚è¾“å‡ºçº¯æ–‡æœ¬å†…å®¹ï¼š\n- åªè¾“å‡ºçº¯æ–‡æœ¬ï¼Œä¸ä½¿ç”¨ä»»ä½•markdownæ ¼å¼ç¬¦å·\n- ä¸ä½¿ç”¨ #ã€*ã€-ã€[] ç­‰markdownæ ‡è®°\n- ä½¿ç”¨é˜¿æ‹‰ä¼¯æ•°å­—å’Œä¸­æ–‡æ ‡ç‚¹ç¬¦å·è¿›è¡Œç»“æ„åŒ–è¾“å‡º\n- é‡è¦å†…å®¹å¯ä»¥ç”¨ä¸­æ–‡æè¿°è¯å¼ºè°ƒï¼Œå¦‚"é‡ç‚¹å…³æ³¨"ã€"éœ€è¦æ³¨æ„"\n- ä¿æŒå†…å®¹ç®€æ´æ˜äº†ï¼Œé¿å…æ ¼å¼æ··ä¹±'
                },
                {
                    'role': 'user',
                    'content': prompt
                }
            ],
            'max_tokens': max_tokens,
            'temperature': 0.7
        }
        
        try:
            response = requests.post(
                f"{self.deepseek_api_base}/chat/completions",
                headers=headers,
                json=data,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                if 'choices' in result and len(result['choices']) > 0:
                    return result['choices'][0]['message']['content']
            else:
                print(f"DeepSeek APIé”™è¯¯: {response.status_code} - {response.text}")
                return None
                
        except requests.exceptions.RequestException as e:
            print(f"DeepSeek APIè¯·æ±‚å¤±è´¥: {e}")
            return None
        except Exception as e:
            print(f"DeepSeek APIè°ƒç”¨å¼‚å¸¸: {e}")
            return None
    
    def analyze_dns_anomalies(self):
        """åˆ†æDNSæŸ¥è¯¢å¼‚å¸¸"""
        if not self.deepseek_api_key:
            return {"status": "no_api_key", "message": "æœªé…ç½®DeepSeek APIå¯†é’¥"}
        
        # è·å–å½“å‰å°æ—¶å’Œæœ€è¿‘1å°æ—¶çš„æ•°æ®
        now = datetime.now()
        current_hour = now.hour
        last_hour = (now - timedelta(hours=1)).hour
        
        # ä»æ•°æ®åº“è·å–å½“å‰æ•°æ®
        today_str = datetime.now().strftime('%Y-%m-%d')
        hourly_stats = self.get_hourly_stats_from_db(1)
        
        # åˆ†ææœ€è¿‘1å°æ—¶çš„æŸ¥è¯¢çªå¢
        current_hour_queries = hourly_stats.get(current_hour, 0)
        last_hour_queries = hourly_stats.get(last_hour, 0)
        
        # è·å–æœ€è¿‘24å°æ—¶æœ€æ´»è·ƒçš„åŸŸå
        top_domains_24h = self.get_top_domains_24h_from_db(20)
        
        # è·å–æŸ¥è¯¢é‡æœ€é«˜çš„6ä¸ªå®¢æˆ·ç«¯åŠå…¶é«˜é¢‘åŸŸå
        top_clients_with_domains = self.get_client_stats_24h_from_db(6)
        
        # è·å–å†å²æ•°æ®è¿›è¡Œå¯¹æ¯”
        multi_day_stats = self.get_multi_day_stats_from_db(7)
        historical_averages = {
            'avg_total_queries': multi_day_stats['total_queries'] / 7,
            'avg_hourly': {h: c / 7 for h, c in multi_day_stats['hourly_stats'].items()},
            'historical_days': 7
        }
        
        # æ„å»ºåˆ†ææç¤º
        prompt = self.build_analysis_prompt(
            current_hour_queries, last_hour_queries, top_domains_24h, 
            historical_averages, current_hour, top_clients_with_domains
        )
        
        # è°ƒç”¨AIåˆ†æ
        ai_analysis = self.call_deepseek_api(prompt)
        
        if ai_analysis:
            return {
                "status": "success",
                "analysis": ai_analysis,
                "timestamp": now.isoformat(),
                "data_summary": {
                    "current_hour_queries": current_hour_queries,
                    "last_hour_queries": last_hour_queries,
                    "top_domains_count": len(top_domains_24h),
                    "total_domains_24h": len(top_domains_24h)
                }
            }
        else:
            return {"status": "api_error", "message": "AIåˆ†æè°ƒç”¨å¤±è´¥"}
    
    
    def build_analysis_prompt(self, current_hour_queries, last_hour_queries, 
                            top_domains, historical_averages, current_hour, top_clients_with_domains=None):
        """æ„å»ºAIåˆ†ææç¤º"""
        
        # è®¡ç®—æŸ¥è¯¢å˜åŒ–ç‡
        if last_hour_queries > 0:
            change_rate = ((current_hour_queries - last_hour_queries) / last_hour_queries) * 100
        else:
            change_rate = 100 if current_hour_queries > 0 else 0
        
        # è·å–å†å²å¹³å‡å€¼è¿›è¡Œå¯¹æ¯”
        hist_avg_current = historical_averages.get('avg_hourly', {}).get(current_hour, 0)
        hist_avg_last = historical_averages.get('avg_hourly', {}).get((current_hour-1) % 24, 0)
        
        prompt = f"""
è¯·åˆ†æä»¥ä¸‹DNSæŸ¥è¯¢æ•°æ®ï¼Œæä¾›æ€åŠ¿æ„ŸçŸ¥æè¿°ï¼š

æ—¶é—´ä¿¡æ¯ï¼š
- å½“å‰æ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} (ç¬¬{current_hour}å°æ—¶)
- åˆ†ææ—¶æ®µï¼šæœ€è¿‘1å°æ—¶

æŸ¥è¯¢é‡ç»Ÿè®¡ï¼š
- å½“å‰å°æ—¶æŸ¥è¯¢é‡ï¼š{current_hour_queries:,} æ¬¡
- ä¸Šä¸€å°æ—¶æŸ¥è¯¢é‡ï¼š{last_hour_queries:,} æ¬¡
- å˜åŒ–ç‡ï¼š{change_rate:+.1f}%

å†å²å¯¹æ¯”ï¼ˆåŸºäºè¿‡å»{historical_averages.get('historical_days', 0)}å¤©æ•°æ®ï¼‰ï¼š
- å½“å‰å°æ—¶å†å²å¹³å‡ï¼š{hist_avg_current:.0f} æ¬¡
- ä¸Šä¸€å°æ—¶å†å²å¹³å‡ï¼š{hist_avg_last:.0f} æ¬¡

æœ€è¿‘24å°æ—¶TOPåŸŸåï¼š
"""
        
        for i, (domain, count) in enumerate(top_domains[:10], 1):
            prompt += f"{i}. {domain}: {count:,} æ¬¡æŸ¥è¯¢\n"
        
        # æ·»åŠ å®¢æˆ·ç«¯åŸŸååˆ†ææ•°æ®
        if top_clients_with_domains:
            prompt += f"\næœ€æ´»è·ƒçš„6ä¸ªå®¢æˆ·ç«¯åŠå…¶é«˜é¢‘åŸŸåï¼š\n"
            for i, client_data in enumerate(top_clients_with_domains, 1):
                client_ip = client_data['client_ip']
                total_queries = client_data['total_queries']
                top_domains_client = client_data['top_domains'][:5]  # åªæ˜¾ç¤ºå‰5ä¸ªåŸŸå
                
                prompt += f"{i}. {client_ip} ({total_queries:,} æ¬¡æŸ¥è¯¢):\n"
                for j, (domain, count) in enumerate(top_domains_client, 1):
                    percentage = (count / total_queries * 100) if total_queries > 0 else 0
                    prompt += f"   {j}. {domain}: {count:,} æ¬¡ ({percentage:.1f}%)\n"
        
        prompt += f"""
è¯·åŸºäºä»¥ä¸Šæ•°æ®æä¾›æ€åŠ¿æ„ŸçŸ¥åˆ†æï¼ŒåŒ…æ‹¬ï¼š
1. æŸ¥è¯¢é‡è¶‹åŠ¿åˆ†æï¼ˆæ˜¯å¦å¼‚å¸¸ï¼‰
2. åŸŸåè®¿é—®æ¨¡å¼è¯†åˆ«
3. å®¢æˆ·ç«¯è¡Œä¸ºåˆ†æï¼ˆæ˜¯å¦æœ‰å¼‚å¸¸é›†ä¸­æˆ–å¯ç–‘æ´»åŠ¨ï¼‰
4. å¯èƒ½çš„å®‰å…¨é£é™©æˆ–å¼‚å¸¸è¡Œä¸º
5. ç®€è¦çš„å®‰å…¨å»ºè®®

è¯·ç”¨ç®€æ´çš„ä¸­æ–‡å›ç­”ï¼Œé‡ç‚¹çªå‡ºå¼‚å¸¸æƒ…å†µå’Œå®‰å…¨å…³æ³¨ç‚¹ã€‚å¦‚æœä¸€åˆ‡æ­£å¸¸ï¼Œè¯·è¯´æ˜å½“å‰ç½‘ç»œæ´»åŠ¨æ­£å¸¸ã€‚è¯·ä½¿ç”¨çº¯æ–‡æœ¬æ ¼å¼è¾“å‡ºï¼Œä¾¿äºç›´æ¥æ˜¾ç¤ºã€‚
"""
        
        return prompt
    
    def analyze_log(self):
        """åˆ†ææ—¥å¿—æ–‡ä»¶å¹¶å†™å…¥æ•°æ®åº“"""
        if not os.path.exists(self.log_file):
            print(f"æ—¥å¿—æ–‡ä»¶ {self.log_file} ä¸å­˜åœ¨")
            return False
        
        new_records = 0
        duplicate_records = 0
        arpa_excluded = 0
        
        conn = self.get_db_connection()
        cursor = conn.cursor()
        
        try:
            # å¼€å§‹äº‹åŠ¡
            conn.execute('BEGIN TRANSACTION')
            
            with open(self.log_file, 'r', encoding='utf-8', errors='ignore') as f:
                for line in f:
                    # ç”Ÿæˆè¡Œçš„å”¯ä¸€æ ‡è¯†
                    line_hash = self.get_line_hash(line.strip())
                    
                    # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†ï¼ˆæŸ¥è¯¢æ•°æ®åº“ï¼‰
                    cursor.execute('''
                        SELECT 1 FROM dns_queries WHERE line_hash = ?
                        UNION ALL
                        SELECT 1 FROM cache_hits WHERE line_hash = ?
                        UNION ALL
                        SELECT 1 FROM dns_forwards WHERE line_hash = ?
                        LIMIT 1
                    ''', (line_hash, line_hash, line_hash))
                    
                    if cursor.fetchone():
                        duplicate_records += 1
                        continue
                    
                    data = self.parse_log_line(line)
                    if data and self.is_within_analysis_window(data['timestamp']):
                        # æ£€æŸ¥æ˜¯å¦ä¸º.arpaåŸŸåå¹¶æ ¹æ®è®¾ç½®å†³å®šæ˜¯å¦æ’é™¤
                        if self.exclude_arpa and self.is_arpa_domain(data['domain']):
                            arpa_excluded += 1
                            continue
                        
                        date_str = data['timestamp'].strftime('%Y-%m-%d')
                        hour = data['timestamp'].hour
                        
                        try:
                            # å¤„ç†æŸ¥è¯¢è®°å½•
                            if data['type'] == 'query':
                                cursor.execute('''
                                    INSERT OR IGNORE INTO dns_queries 
                                    (line_hash, timestamp, query_type, domain, client_ip, record_type, date_only, hour)
                                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                                ''', (line_hash, data['timestamp'], data['type'], data['domain'], 
                                      data['client_ip'], data['query_type'], date_str, hour))
                                
                            # å¤„ç†ç¼“å­˜å‘½ä¸­è®°å½•
                            elif data['type'] == 'cache_hit':
                                cursor.execute('''
                                    INSERT OR IGNORE INTO cache_hits 
                                    (line_hash, timestamp, domain, date_only, hour)
                                    VALUES (?, ?, ?, ?, ?)
                                ''', (line_hash, data['timestamp'], data['domain'], date_str, hour))
                                
                            # å¤„ç†è½¬å‘è®°å½•ï¼ˆç¼“å­˜æœªå‘½ä¸­ï¼‰
                            elif data['type'] == 'forward':
                                cursor.execute('''
                                    INSERT OR IGNORE INTO dns_forwards 
                                    (line_hash, timestamp, domain, upstream_server, date_only, hour)
                                    VALUES (?, ?, ?, ?, ?, ?)
                                ''', (line_hash, data['timestamp'], data['domain'], 
                                      data['upstream'], date_str, hour))
                            
                            if cursor.rowcount > 0:
                                new_records += 1
                                
                        except sqlite3.IntegrityError:
                            # é‡å¤è®°å½•ï¼Œå¿½ç•¥
                            duplicate_records += 1
                            continue
            
            # æäº¤äº‹åŠ¡
            conn.commit()
            
            # è·å–ç»Ÿè®¡æ•°æ®
            stats = self.get_statistics_from_db()
            
            print(f"\nç»Ÿè®¡ç»“æœ:")
            print(f"  æ–°å¢è®°å½•: {new_records} æ¡")
            print(f"  è·³è¿‡é‡å¤: {duplicate_records} æ¡")
            if self.exclude_arpa:
                print(f"  æ’é™¤.arpaæŸ¥è¯¢: {arpa_excluded} æ¡")
            print(f"  æŸ¥è¯¢æ€»æ•°: {stats['total_queries']} æ¡")
            print(f"  ç¼“å­˜å‘½ä¸­: {stats['cache_hits']} æ¬¡")
            print(f"  ç¼“å­˜æœªå‘½ä¸­: {stats['cache_misses']} æ¬¡")
            print(f"  ç¼“å­˜å‘½ä¸­ç‡: {stats['cache_hit_rate']:.2f}%")
            
            # æ˜¾ç¤ºæ•°æ®åº“çŠ¶æ€
            db_size = os.path.getsize(self.db_file) / 1024 / 1024
            print(f"  æ•°æ®åº“å¤§å°: {db_size:.2f} MB")
            
            # ä¿å­˜å¤„ç†çŠ¶æ€
            self.save_state()
            
            return True
            
        except Exception as e:
            # å›æ»šäº‹åŠ¡
            conn.rollback()
            print(f"åˆ†ææ—¥å¿—æ–‡ä»¶å‡ºé”™: {e}")
            return False
        finally:
            conn.close()
    
    
    def cleanup_old_data(self):
        """æ¸…ç†è¿‡æœŸçš„æ•°æ®åº“è®°å½•"""
        try:
            current_time = datetime.now()
            cutoff_date = (current_time - timedelta(days=self.keep_days)).strftime('%Y-%m-%d')
            
            conn = self.get_db_connection()
            cursor = conn.cursor()
            
            # åˆ é™¤è¿‡æœŸè®°å½•
            cursor.execute('DELETE FROM dns_queries WHERE date_only < ?', (cutoff_date,))
            queries_deleted = cursor.rowcount
            
            cursor.execute('DELETE FROM cache_hits WHERE date_only < ?', (cutoff_date,))
            cache_deleted = cursor.rowcount
            
            cursor.execute('DELETE FROM dns_forwards WHERE date_only < ?', (cutoff_date,))
            forwards_deleted = cursor.rowcount
            
            conn.commit()
            
            total_deleted = queries_deleted + cache_deleted + forwards_deleted
            if total_deleted > 0:
                print(f"æ•°æ®æ¸…ç†å®Œæˆ: åˆ é™¤äº† {total_deleted} æ¡è¿‡æœŸè®°å½• (è¶…è¿‡ {self.keep_days} å¤©)")
                print(f"  æŸ¥è¯¢è®°å½•: {queries_deleted} æ¡")
                print(f"  ç¼“å­˜è®°å½•: {cache_deleted} æ¡")
                print(f"  è½¬å‘è®°å½•: {forwards_deleted} æ¡")
                
                # ä¼˜åŒ–æ•°æ®åº“
                cursor.execute('VACUUM')
                conn.commit()
                print("æ•°æ®åº“å·²ä¼˜åŒ–")
            else:
                print(f"æ•°æ®æ¸…ç†æ£€æŸ¥å®Œæˆ: å½“å‰æ‰€æœ‰æ•°æ®éƒ½åœ¨ {self.keep_days} å¤©ä¿ç•™æœŸå†…")
                
            conn.close()
        except Exception as e:
            print(f"æ•°æ®æ¸…ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
    
    
    
    
    
    def generate_html_report(self, output_file='dnsmasq_report.html'):
        """ç”ŸæˆHTMLåˆ†ææŠ¥å‘Š"""
        # ä»æ•°æ®åº“è·å–æ•°æ®
        today_str = datetime.now().strftime('%Y-%m-%d')
        
        # è·å–æœ€è¿‘24å°æ—¶çš„TOPåŸŸå
        top_domains_24h = self.get_top_domains_24h_from_db(50)
        
        # è·å–æŸ¥è¯¢é‡æœ€é«˜çš„6ä¸ªå®¢æˆ·ç«¯åŠå…¶TOP 10åŸŸå
        top_clients_with_domains = self.get_client_stats_24h_from_db(6)
        
        # è·å–24å°æ—¶å’Œå½“å¤©çš„ç»Ÿè®¡æ•°æ®
        stats_24h = self.get_24h_statistics_from_db()
        today_stats = self.get_statistics_from_db(today_str)
        cache_hit_rate = stats_24h['cache_hit_rate']
        
        # è·å–ç¼“å­˜ç»Ÿè®¡æ•°æ®
        cache_stats = self.get_cache_stats_from_db(today_str, 10)
        top_cached = cache_stats['top_cached']
        top_forwarded = cache_stats['top_forwarded']
        upstream_servers = cache_stats['upstream_servers']
        
        # æ‰§è¡ŒAIæ€åŠ¿æ„ŸçŸ¥åˆ†æ
        ai_analysis_result = self.analyze_dns_anomalies()
        
        # è·å–7å¤©çš„ç»Ÿè®¡æ•°æ®
        multi_day_stats = self.get_multi_day_stats_from_db(7)
        all_time_domains = multi_day_stats['top_domains']
        total_queries_7d = multi_day_stats['total_queries']
        total_cache_hits_7d = multi_day_stats['total_cache_hits']
        total_cache_misses_7d = multi_day_stats['total_cache_misses']
        cache_hit_rate_7d = multi_day_stats['cache_hit_rate']
        hourly_stats_7d = multi_day_stats['hourly_stats']
        
        # è·å–24å°æ—¶çš„æŸ¥è¯¢æ€»æ•°
        h24_total_queries = stats_24h['total_queries']
        
        html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DNSmasq æ—¥å¿—åˆ†ææŠ¥å‘Š</title>
    <style>
        * {{
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }}
        
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
        }}
        
        .container {{
            max-width: 1400px;
            margin: 0 auto;
        }}
        
        .header {{
            text-align: center;
            color: white;
            margin-bottom: 30px;
            padding: 20px;
            background: rgba(255, 255, 255, 0.1);
            border-radius: 15px;
            backdrop-filter: blur(10px);
        }}
        
        .header h1 {{
            font-size: 2.5em;
            margin-bottom: 10px;
        }}
        
        .header .update-time {{
            font-size: 1.1em;
            opacity: 0.9;
        }}
        
        .ai-analysis {{
            background: linear-gradient(135deg, #ff9a9e 0%, #fecfef 50%, #fecfef 100%);
            border-radius: 15px;
            padding: 25px;
            margin-bottom: 30px;
            color: #333;
            box-shadow: 0 15px 35px rgba(255, 154, 158, 0.3);
        }}
        
        .ai-analysis h2 {{
            color: #d63384;
            margin-bottom: 15px;
            display: flex;
            align-items: center;
            gap: 10px;
        }}
        
        .ai-analysis .content {{
            background: rgba(255, 255, 255, 0.8);
            border-radius: 10px;
            padding: 20px;
            line-height: 1.6;
        }}
        
        .ai-analysis .content h1, .ai-analysis .content h2, .ai-analysis .content h3, .ai-analysis .content h4 {{
            color: #d63384;
            margin: 15px 0 10px 0;
            font-weight: bold;
        }}
        
        .ai-analysis .content h1 {{ font-size: 1.4em; }}
        .ai-analysis .content h2 {{ font-size: 1.3em; }}
        .ai-analysis .content h3 {{ font-size: 1.2em; }}
        .ai-analysis .content h4 {{ font-size: 1.1em; }}
        
        .ai-analysis .content p {{
            margin: 10px 0;
            text-align: justify;
        }}
        
        .ai-analysis .content ul, .ai-analysis .content ol {{
            margin: 10px 0;
            padding-left: 20px;
        }}
        
        .ai-analysis .content li {{
            margin: 5px 0;
            list-style-type: disc;
        }}
        
        .ai-analysis .content ol li {{
            list-style-type: decimal;
        }}
        
        .ai-analysis .content strong {{
            color: #d63384;
            font-weight: bold;
        }}
        
        .ai-analysis .content em {{
            font-style: italic;
            color: #666;
        }}
        
        .ai-analysis .content code {{
            background: rgba(233, 236, 239, 0.8);
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            color: #e83e8c;
        }}
        
        .ai-analysis .content pre {{
            background: rgba(233, 236, 239, 0.8);
            padding: 15px;
            border-radius: 5px;
            margin: 15px 0;
            overflow-x: auto;
        }}
        
        .ai-analysis .content pre code {{
            background: none;
            padding: 0;
            font-size: 0.9em;
            color: #333;
        }}
        
        .ai-analysis .no-analysis {{
            text-align: center;
            padding: 20px;
            color: #666;
            font-style: italic;
        }}
        
        .ai-analysis .error {{
            background: rgba(220, 53, 69, 0.1);
            border: 1px solid rgba(220, 53, 69, 0.3);
            color: #721c24;
        }}
        
        .stats-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }}
        
        .stat-card {{
            background: white;
            border-radius: 10px;
            padding: 20px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.1);
            transition: transform 0.3s ease;
        }}
        
        .stat-card:hover {{
            transform: translateY(-5px);
        }}
        
        .stat-card h3 {{
            color: #666;
            font-size: 0.9em;
            margin-bottom: 10px;
            text-transform: uppercase;
            letter-spacing: 1px;
        }}
        
        .stat-card .value {{
            font-size: 2.5em;
            font-weight: bold;
            color: #333;
        }}
        
        .stat-card .change {{
            font-size: 0.9em;
            color: #4CAF50;
            margin-top: 5px;
        }}
        
        .main-content {{
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin-bottom: 30px;
        }}
        
        @media (max-width: 968px) {{
            .main-content {{
                grid-template-columns: 1fr;
            }}
        }}
        
        .card {{
            background: white;
            border-radius: 10px;
            padding: 25px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.1);
        }}
        
        .card h2 {{
            color: #333;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid #f0f0f0;
        }}
        
        .domain-list {{
            max-height: 500px;
            overflow-y: auto;
        }}
        
        .domain-item {{
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 12px;
            margin-bottom: 8px;
            background: #f8f9fa;
            border-radius: 8px;
            transition: background 0.3s ease;
        }}
        
        .domain-item:hover {{
            background: #e9ecef;
        }}
        
        .domain-rank {{
            display: inline-block;
            width: 30px;
            height: 30px;
            line-height: 30px;
            text-align: center;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border-radius: 50%;
            font-weight: bold;
            margin-right: 15px;
        }}
        
        .domain-name {{
            flex: 1;
            font-weight: 500;
            color: #333;
            word-break: break-all;
        }}
        
        .domain-count {{
            background: #667eea;
            color: white;
            padding: 5px 12px;
            border-radius: 20px;
            font-weight: bold;
            font-size: 0.9em;
        }}
        
        .chart-container {{
            margin-top: 20px;
            height: 300px;
            position: relative;
        }}
        
        .hourly-chart {{
            display: flex;
            align-items: flex-end;
            height: 250px;
            padding: 10px 0;
            border-left: 2px solid #ddd;
            border-bottom: 2px solid #ddd;
        }}
        
        .hour-bar {{
            flex: 1;
            background: linear-gradient(to top, #667eea, #764ba2);
            margin: 0 2px;
            border-radius: 4px 4px 0 0;
            position: relative;
            transition: opacity 0.3s ease;
        }}
        
        .hour-bar:hover {{
            opacity: 0.8;
        }}
        
        .hour-bar::after {{
            content: attr(data-hour);
            position: absolute;
            bottom: -20px;
            left: 50%;
            transform: translateX(-50%);
            font-size: 10px;
            color: #666;
        }}
        
        .hour-bar::before {{
            content: attr(data-count);
            position: absolute;
            top: -20px;
            left: 50%;
            transform: translateX(-50%);
            font-size: 10px;
            color: #333;
            font-weight: bold;
        }}
        
        .info-grid {{
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin-top: 20px;
        }}
        
        .info-item {{
            padding: 15px;
            background: #f8f9fa;
            border-radius: 8px;
        }}
        
        .info-item h4 {{
            color: #666;
            margin-bottom: 8px;
            font-size: 0.9em;
        }}
        
        .info-item .value {{
            color: #333;
            font-size: 1.2em;
            font-weight: bold;
        }}
        
        .footer {{
            text-align: center;
            color: white;
            margin-top: 40px;
            padding: 20px;
            background: rgba(255, 255, 255, 0.1);
            border-radius: 10px;
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸŒ DNSmasq æ—¥å¿—åˆ†ææŠ¥å‘Š</h1>
            <div class="update-time">æ›´æ–°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</div>
        </div>
        
        <!-- AIæ€åŠ¿æ„ŸçŸ¥åˆ†æ -->
        <div class="ai-analysis">
            <h2>ğŸ¤– AIæ€åŠ¿æ„ŸçŸ¥åˆ†æ</h2>"""

        if ai_analysis_result['status'] == 'success':
            # å¤„ç†çº¯æ–‡æœ¬æ ¼å¼çš„AIåˆ†æç»“æœ
            analysis_text = ai_analysis_result['analysis'].replace('\n', '<br>')
            html_content += f"""
            <div class="content"><p>{analysis_text}</p></div>"""
        elif ai_analysis_result['status'] == 'no_api_key':
            html_content += f"""
            <div class="no-analysis">
                ğŸ’¡ æœªé…ç½®DeepSeek APIå¯†é’¥ï¼Œæ— æ³•è¿›è¡ŒAIåˆ†æ<br>
                è¿è¡Œ <code>python3 dnsmasq_analyzer.py --setup-ai</code> è¿›è¡Œé…ç½®
            </div>"""
        else:
            html_content += f"""
            <div class="content error">
                âš ï¸ AIåˆ†ææš‚æ—¶ä¸å¯ç”¨: {ai_analysis_result.get('message', 'æœªçŸ¥é”™è¯¯')}<br>
                è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé…ç½®
            </div>"""
        
        html_content += f"""
        </div>
        
        <div class="stats-grid">
            <div class="stat-card">
                <h3>24å°æ—¶æŸ¥è¯¢æ€»æ•°</h3>
                <div class="value">{h24_total_queries:,}</div>
            </div>
            <div class="stat-card">
                <h3>24å°æ—¶ç¼“å­˜å‘½ä¸­ç‡</h3>
                <div class="value">{cache_hit_rate:.1f}%</div>
                <div class="change">å‘½ä¸­:{stats_24h['cache_hits']:,} / æœªä¸­:{stats_24h['cache_misses']:,}</div>
            </div>
            <div class="stat-card">
                <h3>ç‹¬ç«‹åŸŸåæ•°</h3>
                <div class="value">{len(top_domains_24h):,}</div>
            </div>
            <div class="stat-card">
                <h3>æ´»è·ƒå®¢æˆ·ç«¯</h3>
                <div class="value">{len(top_clients_with_domains):,}</div>
            </div>
            <div class="stat-card">
                <h3>7å¤©æŸ¥è¯¢æ€»æ•°</h3>
                <div class="value">{total_queries_7d:,}</div>
            </div>
            <div class="stat-card">
                <h3>7å¤©ç¼“å­˜å‘½ä¸­ç‡</h3>
                <div class="value">{cache_hit_rate_7d:.1f}%</div>
                <div class="change">å‘½ä¸­:{total_cache_hits_7d:,} / æœªä¸­:{total_cache_misses_7d:,}</div>
            </div>
        </div>
        
        <div class="main-content">
            <div class="card">
                <h2>ğŸ“Š æœ€è¿‘24å°æ—¶é«˜é¢‘è®¿é—®åŸŸå TOP 50</h2>
                <div class="domain-list">
"""
        
        # æ·»åŠ TOPåŸŸååˆ—è¡¨
        for idx, (domain, count) in enumerate(top_domains_24h, 1):
            html_content += f"""
                    <div class="domain-item">
                        <span class="domain-rank">{idx}</span>
                        <span class="domain-name">{domain}</span>
                        <span class="domain-count">{count:,}</span>
                    </div>
"""
        
        html_content += """
                </div>
            </div>
            
            <div class="card">
                <h2>ğŸ“ˆ 7å¤©ç´¯è®¡24å°æ—¶æŸ¥è¯¢æ—¶é—´åˆ†å¸ƒ</h2>
                <div class="chart-container">
                    <div class="hourly-chart">
"""
        
        # æ·»åŠ å°æ—¶åˆ†å¸ƒå›¾ï¼ˆä½¿ç”¨7å¤©ç´¯è®¡æ•°æ®ï¼‰
        max_hourly = max(hourly_stats_7d.values()) if hourly_stats_7d else 1
        for hour in range(24):
            count = hourly_stats_7d.get(hour, 0)
            height_percent = (count / max_hourly * 100) if max_hourly > 0 else 0
            html_content += f"""
                        <div class="hour-bar" style="height: {height_percent}%;" data-hour="{hour:02d}" data-count="{count}"></div>
"""
        
        html_content += ("""
                    </div>
                </div>
                
                <div class="info-grid">
                    <div class="info-item">
                        <h4>æœ€æ´»è·ƒæ—¶æ®µ</h4>
                        <div class="value">{}</div>
                    </div>
                    <div class="info-item">
                        <h4>å¹³å‡æ¯å°æ—¶æŸ¥è¯¢</h4>
                        <div class="value">{}</div>
                    </div>
                    <div class="info-item">
                        <h4>7å¤©æ€»æŸ¥è¯¢</h4>
                        <div class="value">{}</div>
                    </div>
                    <div class="info-item">
                        <h4>æœ€æ´»è·ƒå®¢æˆ·ç«¯</h4>
                        <div class="value">{}</div>
                    </div>
                </div>
            </div>
        </div>
        """).format(
            f"{max(hourly_stats_7d, key=hourly_stats_7d.get, default=0):02d}:00" if hourly_stats_7d else "N/A",
            f"{total_queries_7d // 24:,}" if total_queries_7d else "0",
            f"{total_queries_7d:,}" if total_queries_7d else "0",
            top_clients_with_domains[0]['client_ip'] if top_clients_with_domains else "N/A"
        )
        
        # TOP 6 å®¢æˆ·ç«¯åŠå…¶é«˜é¢‘åŸŸå
        html_content += """
        <div class="card" style="grid-column: 1 / -1;">
            <h2>ğŸ”¥ æŸ¥è¯¢é‡æœ€é«˜çš„6ä¸ªå®¢æˆ·ç«¯åŠå…¶TOP 10åŸŸå</h2>
            <div class="main-content" style="grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));">
"""
        
        # æ·»åŠ æ¯ä¸ªå®¢æˆ·ç«¯çš„å¡ç‰‡
        for idx, client_data in enumerate(top_clients_with_domains, 1):
            client_ip = client_data['client_ip']
            total_queries = client_data['total_queries']
            top_domains = client_data['top_domains']
            
            html_content += f"""
                <div class="card" style="margin: 0;">
                    <h3 style="color: #667eea; margin-bottom: 15px; display: flex; align-items: center; gap: 10px;">
                        <span class="domain-rank" style="font-size: 14px;">{idx}</span>
                        {client_ip}
                        <span style="font-size: 14px; color: #666; font-weight: normal;">({total_queries:,} æ¬¡æŸ¥è¯¢)</span>
                    </h3>
                    <div class="domain-list" style="max-height: 350px;">
"""
            
            # æ·»åŠ è¯¥å®¢æˆ·ç«¯çš„TOPåŸŸå
            for domain_idx, (domain, count) in enumerate(top_domains, 1):
                percentage = (count / total_queries * 100) if total_queries > 0 else 0
                html_content += f"""
                        <div class="domain-item">
                            <span class="domain-rank" style="width: 25px; height: 25px; line-height: 25px; font-size: 12px;">{domain_idx}</span>
                            <span class="domain-name" style="font-size: 14px;">{domain}</span>
                            <div style="display: flex; flex-direction: column; align-items: flex-end;">
                                <span class="domain-count" style="background: #28a745;">{count:,}</span>
                                <small style="color: #666; font-size: 11px; margin-top: 2px;">{percentage:.1f}%</small>
                            </div>
                        </div>
"""
            
            html_content += """
                    </div>
                </div>
"""
        
        html_content += """
            </div>
        </div>
        
        <div class="card" style="grid-column: 1 / -1;">
            <h2>ğŸ’¾ ç¼“å­˜æ€§èƒ½åˆ†æ</h2>
            <div class="main-content">
                <div>
                    <h3 style="color: #666; margin-bottom: 15px;">ğŸ”¥ ç¼“å­˜å‘½ä¸­æœ€å¤šçš„åŸŸå TOP 10</h3>
                    <div class="domain-list" style="max-height: 300px;">
"""
        
        # æ·»åŠ ç¼“å­˜å‘½ä¸­TOPåŸŸå
        for idx, (domain, count) in enumerate(top_cached, 1):
            html_content += f"""
                        <div class="domain-item">
                            <span class="domain-rank">{idx}</span>
                            <span class="domain-name">{domain}</span>
                            <span class="domain-count" style="background: #4CAF50;">{count:,}</span>
                        </div>
"""
        
        html_content += """
                    </div>
                </div>
                <div>
                    <h3 style="color: #666; margin-bottom: 15px;">ğŸ”„ è½¬å‘æ¬¡æ•°æœ€å¤šçš„åŸŸå TOP 10</h3>
                    <div class="domain-list" style="max-height: 300px;">
"""
        
        # æ·»åŠ è½¬å‘TOPåŸŸå
        for idx, (domain, count) in enumerate(top_forwarded, 1):
            html_content += f"""
                        <div class="domain-item">
                            <span class="domain-rank">{idx}</span>
                            <span class="domain-name">{domain}</span>
                            <span class="domain-count" style="background: #FF9800;">{count:,}</span>
                        </div>
"""
        
        html_content += """
                    </div>
                </div>
            </div>
            
            <div class="info-grid" style="margin-top: 20px;">
                <div class="info-item">
                    <h4>ä¸Šæ¸¸DNSæœåŠ¡å™¨ä½¿ç”¨æƒ…å†µ</h4>
                    <div class="value" style="font-size: 1em;">
"""
        
        # æ·»åŠ ä¸Šæ¸¸æœåŠ¡å™¨ç»Ÿè®¡
        for server, count in upstream_servers[:5]:
            html_content += f"                        {server}: {count:,} æ¬¡<br>"
        
        html_content += """
                    </div>
                </div>
                <div class="info-item">
                    <h4>ç¼“å­˜æ•ˆç‡æå‡</h4>
                    <div class="value">{:.1f}%</div>
                    <small style="color: #666;">å‡å°‘äº† {:.0f}% çš„ä¸Šæ¸¸æŸ¥è¯¢</small>
                </div>
            </div>
        </div>
""".format(cache_hit_rate, cache_hit_rate)
        
        # æ·»åŠ 7å¤©TOPåŸŸå
        html_content += """
        <div class="card" style="grid-column: 1 / -1;">
            <h2>ğŸ† æœ€è¿‘7å¤©é«˜é¢‘è®¿é—®åŸŸå TOP 50</h2>
            <div class="domain-list" style="column-count: 2; column-gap: 20px;">
"""
        
        for idx, (domain, count) in enumerate(all_time_domains, 1):
            html_content += f"""
                <div class="domain-item" style="break-inside: avoid;">
                    <span class="domain-rank">{idx}</span>
                    <span class="domain-name">{domain}</span>
                    <span class="domain-count">{count:,}</span>
                </div>
"""
        
        html_content += """
            </div>
        </div>
        
        <div class="footer">
            <p>DNSmasq Analyzer v1.0 | æ•°æ®æ¯æ—¥è‡ªåŠ¨æ›´æ–°</p>
            <p>æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {}</p>
        </div>
    </div>
</body>
</html>
""".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        print(f"HTMLæŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")

def main():
    parser = argparse.ArgumentParser(description='DNSmasqæ—¥å¿—åˆ†æå·¥å…·')
    parser.add_argument('-l', '--log', default='/var/log/dnsmasq.log', 
                       help='DNSmasqæ—¥å¿—æ–‡ä»¶è·¯å¾„ (é»˜è®¤: /var/log/dnsmasq.log)')
    parser.add_argument('-o', '--output', default='dnsmasq_report.html',
                       help='è¾“å‡ºHTMLæŠ¥å‘Šæ–‡ä»¶å (é»˜è®¤: dnsmasq_report.html)')
    parser.add_argument('-d', '--data-dir', default='./dnsmasq_data',
                       help='å†å²æ•°æ®å­˜å‚¨ç›®å½• (é»˜è®¤: ./dnsmasq_data)')
    parser.add_argument('--keep-days', type=int, default=30,
                       help='æ•°æ®æ–‡ä»¶ä¿ç•™å¤©æ•° (é»˜è®¤: 30å¤©)')
    parser.add_argument('--cleanup-only', action='store_true',
                       help='ä»…æ‰§è¡Œæ•°æ®æ¸…ç†ï¼Œä¸è¿›è¡Œæ—¥å¿—åˆ†æ')
    parser.add_argument('--include-arpa', action='store_true',
                       help='åŒ…å«.arpaåŸŸåæŸ¥è¯¢ (é»˜è®¤æ’é™¤åå‘DNSæŸ¥è¯¢)')
    parser.add_argument('--setup-ai', action='store_true',
                       help='é…ç½®DeepSeek AIåˆ†æåŠŸèƒ½')
    parser.add_argument('--api-key', type=str,
                       help='ç›´æ¥è®¾ç½®DeepSeek APIå¯†é’¥')
    parser.add_argument('--test-ai', action='store_true',
                       help='æµ‹è¯•DeepSeek AIè¿æ¥')
    
    args = parser.parse_args()
    
    print("=" * 50)
    print("DNSmasq æ—¥å¿—åˆ†æå·¥å…·")
    print("=" * 50)
    
    analyzer = DnsmasqAnalyzer(log_file=args.log, data_dir=args.data_dir, keep_days=args.keep_days, exclude_arpa=not args.include_arpa)
    
    # å‘½ä»¤è¡Œç›´æ¥è®¾ç½®APIå¯†é’¥
    if args.api_key:
        print("\næ­£åœ¨è®¾ç½®DeepSeek APIå¯†é’¥...")
        if analyzer.setup_deepseek_config(api_key=args.api_key):
            print("âœ… APIå¯†é’¥è®¾ç½®æˆåŠŸ!")
        else:
            print("âŒ APIå¯†é’¥è®¾ç½®å¤±è´¥!")
        return
    
    # AIé…ç½®æ¨¡å¼ï¼ˆäº¤äº’å¼ï¼‰
    if args.setup_ai:
        print("\næ­£åœ¨é…ç½®DeepSeek AIåˆ†æåŠŸèƒ½...")
        if analyzer.setup_deepseek_config():
            print("âœ… AIåŠŸèƒ½é…ç½®å®Œæˆ!")
        else:
            print("âŒ AIåŠŸèƒ½é…ç½®å¤±è´¥!")
        return
    
    # AIæµ‹è¯•æ¨¡å¼
    if args.test_ai:
        print("\næ­£åœ¨æµ‹è¯•DeepSeek AIè¿æ¥...")
        if not analyzer.deepseek_api_key:
            print("âŒ æœªé…ç½®APIå¯†é’¥ï¼Œè¯·å…ˆè¿è¡Œ --setup-ai è¿›è¡Œé…ç½®")
            return
        
        test_prompt = "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹DNSåè®®çš„ä½œç”¨ï¼Œç”¨ä¸€å¥è¯å›ç­”ã€‚"
        result = analyzer.call_deepseek_api(test_prompt, max_tokens=100)
        
        if result:
            print("âœ… DeepSeek APIè¿æ¥æˆåŠŸ!")
            print(f"æµ‹è¯•å“åº”: {result}")
        else:
            print("âŒ DeepSeek APIè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥å’Œç½‘ç»œè¿æ¥")
        return
    
    # å¦‚æœåªæ˜¯æ¸…ç†æ¨¡å¼
    if args.cleanup_only:
        print(f"\næ­£åœ¨æ¸…ç†è¶…è¿‡ {args.keep_days} å¤©çš„æ•°æ®æ–‡ä»¶...")
        analyzer.cleanup_old_data()
        # æ˜¾ç¤ºæ•°æ®åº“å¤§å°
        db_size = analyzer.db_file.stat().st_size / 1024 / 1024 if analyzer.db_file.exists() else 0
        print(f"æ¸…ç†å®Œæˆï¼Œå½“å‰æ•°æ®åº“å¤§å°: {db_size:.2f} MB")
        return
    
    # åˆ†ææ—¥å¿—
    print("\næ­£åœ¨åˆ†ææ—¥å¿—æ–‡ä»¶...")
    if analyzer.analyze_log():
        # ç”ŸæˆæŠ¥å‘Š
        print("\næ­£åœ¨ç”ŸæˆHTMLæŠ¥å‘Š...")
        analyzer.generate_html_report(output_file=args.output)
        
        print("\nâœ… åˆ†æå®Œæˆ!")
        # è·å–ä»Šå¤©çš„ç»Ÿè®¡æ•°æ®
        today_stats = analyzer.get_statistics_from_db()
        print(f"ğŸ“Š å…±åˆ†æ {today_stats['total_queries']} æ¡æŸ¥è¯¢è®°å½•")
        print(f"ğŸ“ æ•°æ®åº“ä¿å­˜åœ¨: {analyzer.db_file}")
        print(f"ğŸ“„ HTMLæŠ¥å‘Š: {args.output}")
        
        # AIåŠŸèƒ½çŠ¶æ€æç¤º
        if analyzer.deepseek_api_key:
            print("ğŸ¤– AIæ€åŠ¿æ„ŸçŸ¥åˆ†æå·²é›†æˆåˆ°HTMLæŠ¥å‘Šä¸­")
        else:
            print("ğŸ’¡ æœªé…ç½®AIåŠŸèƒ½ï¼Œè¿è¡Œ 'python3 dnsmasq_analyzer.py --setup-ai' å¯ç”¨æ™ºèƒ½åˆ†æ")
    else:
        print("\nâŒ åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æœ‰è¯»å–æƒé™")
        sys.exit(1)

if __name__ == "__main__":
    main()